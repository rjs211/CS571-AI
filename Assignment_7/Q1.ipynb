{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re, random\n",
    "from copy import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/shikhar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download nltk stopwords corpus.\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file line by line and clean the text of punctuation.\n",
    "with open('all_sentiment_shuffled.txt', 'r') as file:\n",
    "    data = file.readlines()\n",
    "    data = [re.sub(r'([^\\w\\s]|[0-9])', ' ', line) for line in data]\n",
    "    data = [re.sub(r'(\\s+)', ' ', line) for line in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data column-wise.\n",
    "split_data = [(line2[0], line2[1], line2[2], line2[3:]) for line2 in [line1.strip().split() for line1 in data]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the text and label data, and clear the stopwords from the text.\n",
    "X = [line[3] for line in split_data]\n",
    "Y = [line[1] for line in split_data]\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add('')\n",
    "\n",
    "X = [[w for w in words if w not in stop_words] for words in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for generating the bag-of-words vocabulary.\n",
    "def get_vocab(XData):\n",
    "    vocab = set()\n",
    "\n",
    "    for line in XData:\n",
    "        for word in line:\n",
    "            vocab.add(word)\n",
    "            \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for generating hashmap with word counts in individual documents.\n",
    "def document_set(document):\n",
    "    document_with_counts = {}\n",
    "\n",
    "    for word in document:\n",
    "        if word not in document_with_counts.keys():\n",
    "            document_with_counts[word] = 1\n",
    "        else:\n",
    "            document_with_counts[word] += 1\n",
    "\n",
    "    return document_with_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating the prior and observed probablity values.\n",
    "def TrainNaiveBayes(XTrain, YTrain, alpha = 1):\n",
    "    prior = {}\n",
    "    vocabulary = get_vocab(XTrain)\n",
    "    prob_word_given_class = {}\n",
    "    classes = set(YTrain)\n",
    "\n",
    "    for c in classes:\n",
    "        prior[c] = np.log(len([y for y in YTrain if y == c]) / len(YTrain))\n",
    "        class_documents = [doc for doc, label in zip(XTrain, YTrain) if label == c]\n",
    "        class_documents_with_count = [document_set(doc) for doc in class_documents]\n",
    "        total_word_count = sum([len(doc) for doc in class_documents])\n",
    "        prob_word_given_class[c] = {}\n",
    "\n",
    "        for word in vocabulary:\n",
    "            word_occurences = 0\n",
    "\n",
    "            for doc in class_documents_with_count:\n",
    "                if word in doc.keys():\n",
    "                    word_occurences += doc[word]\n",
    "\n",
    "            prob_word_given_class[c][word] = np.log((word_occurences + alpha) / (total_word_count + alpha * len(vocabulary)))\n",
    "    \n",
    "    return prior, prob_word_given_class, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for predicting the label of a given text given the\n",
    "# previously generated probability values.\n",
    "def PredNaiveBayes(XTest, prior, prob_word_given_class, vocabulary):\n",
    "    pred_labels = []\n",
    "    \n",
    "    for line in XTest:\n",
    "        posterior = {}\n",
    "        max_line = -float('inf')\n",
    "        argmax_line = None\n",
    "\n",
    "        for c in prior.keys():\n",
    "            posterior[c] = prior[c]    \n",
    "            \n",
    "            for word in line:\n",
    "                if word in vocabulary:\n",
    "                    posterior[c] += prob_word_given_class[c][word]\n",
    "\n",
    "            if max_line < posterior[c]:\n",
    "                max_line = posterior[c]\n",
    "                argmax_line = c\n",
    "        \n",
    "        pred_labels.append(argmax_line)\n",
    "    \n",
    "    return pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating the different scores of the prediction.\n",
    "def get_scores(ytrue, ypred):\n",
    "    POS_CLASS, NEG_CLASS = 'pos', 'neg'\n",
    "    true_positives = len([1 for a, b in zip(ytrue, ypred) if a == POS_CLASS and b == POS_CLASS])\n",
    "    false_positives = len([1 for a, b in zip(ytrue, ypred) if a == NEG_CLASS and b == POS_CLASS])\n",
    "    true_negatives = len([1 for a, b in zip(ytrue, ypred) if a == NEG_CLASS and b == NEG_CLASS])\n",
    "    false_negatives = len([1 for a, b in zip(ytrue, ypred) if a == POS_CLASS and b == NEG_CLASS])\n",
    "    \n",
    "    acc = (true_positives + true_negatives) / len(ytrue)\n",
    "    if (true_positives + false_positives) != 0:\n",
    "        prec = (true_positives) / (true_positives + false_positives)\n",
    "    else:\n",
    "        prec = float('nan')\n",
    "    if (true_positives + false_negatives) != 0:\n",
    "        rec = (true_positives) / (true_positives + false_negatives)\n",
    "    else:\n",
    "        rec = float('nan')\n",
    "    if (prec + rec) != 0:\n",
    "        f1 = 2. * prec * rec / (prec + rec)\n",
    "    else:\n",
    "        f1 = float('nan')\n",
    "    \n",
    "    return acc, prec, rec, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for training and testing out the entire classifier pipeline.\n",
    "def TrainTestNaiveBayes(XTrain, YTrain, XTest, YTest):\n",
    "    prior, prob_word_given_class, vocabulary = TrainNaiveBayes(XTrain, YTrain)\n",
    "    YPred = PredNaiveBayes(XTest, prior, prob_word_given_class, vocabulary)\n",
    "    return get_scores(YTest, YPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing the data into different structures.\n",
    "data = list(zip(X, Y))\n",
    "random.shuffle(data)\n",
    "X, Y = [d[0] for d in data], [d[1] for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the variables for k-fold cross validation.\n",
    "num_folds = 5\n",
    "split_size = round(len(X) / num_folds + 0.5)\n",
    "X_splits = []\n",
    "Y_splits = []\n",
    "\n",
    "for i in range(num_folds):\n",
    "    X_splits.append(X[i * split_size: (i + 1) * split_size])\n",
    "    Y_splits.append(Y[i * split_size: (i + 1) * split_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2383, 2383)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the size of the individual k-fold split.\n",
    "len(X_splits[0]), len(Y_splits[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8051031075491701 Precision: 0.8148028578656576, Recall: 0.7931738157182011, F1 Score: 0.8038054237416268\n"
     ]
    }
   ],
   "source": [
    "# Running the classifier on the full dataset and calculating the mean\n",
    "# accuracy, precision, recall and F1 scores.\n",
    "all_scores = [0, 0, 0, 0]\n",
    "\n",
    "for fold in range(num_folds):\n",
    "    XTrain = copy(X_splits)\n",
    "    del XTrain[fold]\n",
    "    XTrain = sum(XTrain, [])\n",
    "    XTest = X_splits[fold]\n",
    "    YTrain = copy(Y_splits)\n",
    "    del YTrain[fold]\n",
    "    YTrain = sum(YTrain, [])\n",
    "    YTest = Y_splits[fold]\n",
    "    scores = TrainTestNaiveBayes(XTrain, YTrain, XTest, YTest)\n",
    "\n",
    "    for i, elem in enumerate(scores):\n",
    "        all_scores[i] += elem / num_folds\n",
    "\n",
    "print('Accuracy: {} Precision: {}, Recall: {}, F1 Score: {}'.format(*all_scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
