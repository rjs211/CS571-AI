{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Import the required libraries.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re, random\n",
    "from copy import copy, deepcopy\n",
    "import sys\n",
    "from collections import Counter\n",
    "import queue\n",
    "from scipy import stats\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "np.random.seed(21)\n",
    "random.seed(21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fz = frozenset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def compute_entropy(labels):\n",
    "    entropy = 0.0\n",
    "    totSamples = len(labels)\n",
    "    labelSet = set(labels.reshape(-1))\n",
    "    for label in labelSet:\n",
    "        prob = np.sum(labels == label) / totSamples\n",
    "        if prob > 1e-12:\n",
    "            entropy -= np.log(prob) * prob\n",
    "\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_entropy(mask, labels):\n",
    "    attr_split_info = 0\n",
    "    attr_count = dict()\n",
    "    for attr_val in set(data_i.reshape(-1)):\n",
    "        ids = np.where(data_i == attr_val)[0]\n",
    "        attr_count[attr_val] = len(ids)\n",
    "        attr_split_info += attr_count[attr_val] * compute_entropy(labels[ids])\n",
    "    return attr_split_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def getBestRuleCN2(all_cond,dataset,labels):\n",
    "    # aim :to reduce entropy. ie choose the one with minimum entropy.\n",
    "    # also gain must be positive. so the weighted entropy must be lesser than old entropy.\n",
    "    # condition of frm (idx, val) and evaluated at dataset[:,idx] == val\n",
    "    min_significant = 10\n",
    "    max_rules_count = 10 # to use in beam search\n",
    "    old_entopy = compute_entropy(labels)\n",
    "    min_entropy = float('inf')\n",
    "    best_rule_set = None\n",
    "    best_rule_mask = None\n",
    "    candidates = {} # candidates are of the form {ruleset : (entropy,mask)} \n",
    "    # initial ruleset empty, inital mask: all true initial entropy: old entropy\n",
    "    # Done: give initial candidates\n",
    "    emp_set = set()\n",
    "    emp_set = fz(emp_set)\n",
    "    all_true_mask = np.asarray([True]*dataset.shape[0],dtype=np.bool)\n",
    "    candidates[emp_set] = (old_entopy, all_true_mask) \n",
    "    \n",
    "    while len(candidates)!= 0:\n",
    "        next_candidates = dict()\n",
    "        for rule_fz, tup in candidates.items():\n",
    "            rule_set = set(rule_set)\n",
    "            rule_mask = tup[1]\n",
    "            rule_entropy = tup[0]\n",
    "            rule_attr = set([cond[0] for cond in rule_set])\n",
    "            for new_cond in all_cond:\n",
    "                if new_cond[0] in rule_attr: # preventing collisions /inconsistencies {A_i = v1, A_i = v2}\n",
    "                    continue\n",
    "                new_rule_mask = rule_mask & dataset[:,new_cond[0]] == new_cond[1]\n",
    "                if np.sum > min_significant:  # checking significance\n",
    "                    new_rule_entropy = get_entropy(new_rule_mask,labels)\n",
    "                    new_rule = deepcopy(rule_set)\n",
    "                    new_rule.add(new_cond)\n",
    "                    new_rule = fz(new_rule)\n",
    "                    next_candidates[new_rule] = (new_rule_entropy, new_rule_mask) # map takes care of dupelicates\n",
    "                    if(new_rule_entropy < min_entropy): # is this right? can entropy decrease later? yes hence beam search. corrected.\n",
    "                        min_entropy = new_rule_entropy\n",
    "                        best_rule_set = new_rule\n",
    "                        best_rule_mask = new_rule_mask\n",
    "                        # add to next candidates, check for best\n",
    "        sort_next_candidates = [(k, d[k]) for k in sorted(d, key=d.get)]\n",
    "        sort_next_candidates = sort_next_candidates[:max_rules_count]\n",
    "        candidates= {}\n",
    "        for k, v in sort_next_candidates:\n",
    "            candidates[k] = v\n",
    "    if min_entropy < old_entopy:\n",
    "        return best_rule_set, best_rule_mask\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def CN2_train(dataset,labels,all_conds):\n",
    "    # ordered rules is followed\n",
    "    rule_list = []\n",
    "    dataset = deepcopy(dataset)\n",
    "    labels = deepcopy(labels)\n",
    "    while(len(labels.reshape(-1)) != 0):\n",
    "        next_rule_set = getBestRuleCN2(all_conds, dataset, labels)\n",
    "        if next_rule_set is None:\n",
    "            break\n",
    "        \n",
    "        to_delete = next_rule_set[1]\n",
    "        to_keep = (to_delete == False)\n",
    "        delete_labels = labels[to_delete]\n",
    "        majority_class = Counter(delete_labels.reshape(-1)).most_common(1)[0][0]\n",
    "        rule_list.append((next_rule_set[0],majority_class))\n",
    "        dataset = dataset[to_keep]\n",
    "        labels = labels[to_keep]\n",
    "    return rule_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_to_check_tuples(rile_list):\n",
    "    to_check_tuples = []\n",
    "    for rule,maj_class in rule_list:\n",
    "        attr_list = np.asarray([d[0] for d in rule], dtype=np.int32)\n",
    "        val_list = np.asarray([d[1] for d in rule])\n",
    "        to_check_tuples.append( (attr_list, val_list, maj_class))\n",
    "    return to_check_tuples\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CN2_infer(dataset,rule_list, default_class = -1):\n",
    "    labels = np.zeros((dataset.shape[0],), dtype = np.int32)\n",
    "    labels += default_class;\n",
    "    to_check_tuples = get_to_check_tuples(rule_list)\n",
    "    for idx, sample in enumerate(dataset):\n",
    "        for attr, val, maj_cls in to_check_tuples:\n",
    "            if np.all(sample[attr] == val):\n",
    "                labels[idx] = maj_cls\n",
    "                break;\n",
    "    return labels\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
