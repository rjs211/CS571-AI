{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re, random\n",
    "from copy import copy, deepcopy\n",
    "import sys\n",
    "from collections import Counter\n",
    "import queue\n",
    "from scipy import stats\n",
    "from scipy.special import comb\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "np.random.seed(21)\n",
    "from sklearn.model_selection import train_test_split\n",
    "random.seed(21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fz = frozenset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_entropy(labels):\n",
    "    entropy = 0.0\n",
    "    totSamples = len(labels)\n",
    "    labelSet = set(labels.reshape(-1))\n",
    "    for label in labelSet:\n",
    "        prob = np.sum(labels == label) / totSamples\n",
    "        if prob > 1e-12:\n",
    "            entropy -= np.log(prob) * prob\n",
    "\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entropy(data_i, labels):\n",
    "    attr_split_info = 0\n",
    "    attr_count = dict()\n",
    "    for attr_val in set(data_i.reshape(-1)):\n",
    "        ids = np.where(data_i == attr_val)[0]\n",
    "        attr_count[attr_val] = len(ids)\n",
    "        attr_split_info += attr_count[attr_val] * compute_entropy(labels[ids])\n",
    "    attr_split_info /= len(data_i)\n",
    "    return attr_split_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBestRuleCN2(all_cond,dataset,labels):\n",
    "    # aim :to reduce entropy. ie choose the one with minimum entropy.\n",
    "    # also gain must be positive. so the weighted entropy must be lesser than old entropy.\n",
    "    # condition of frm (idx, val) and evaluated at dataset[:,idx] == val\n",
    "    min_significant = 2\n",
    "    max_rules_count = 10 # to use in beam search\n",
    "    old_entopy = compute_entropy(labels)\n",
    "    min_entropy = float('inf')\n",
    "    best_rule_set = None\n",
    "    best_rule_mask = None\n",
    "    candidates = {} # candidates are of the form {ruleset : (entropy,mask)} \n",
    "    # initial ruleset empty, inital mask: all true initial entropy: old entropy\n",
    "    # Done: give initial candidates\n",
    "    emp_set = set()\n",
    "    emp_set = fz(emp_set)\n",
    "    all_true_mask = np.asarray([True]*dataset.shape[0],dtype=np.bool)\n",
    "    candidates[emp_set] = (old_entopy, all_true_mask) \n",
    "    \n",
    "    while len(candidates)!= 0:\n",
    "        next_candidates = dict()\n",
    "        for rule_fz, tup in candidates.items():\n",
    "            rule_set = set(rule_fz)\n",
    "            rule_mask = tup[1]\n",
    "            rule_entropy = tup[0]\n",
    "            rule_attr = set([cond[0] for cond in rule_set])\n",
    "            for new_cond in all_cond:\n",
    "                if new_cond[0] in rule_attr: # preventing collisions /inconsistencies {A_i = v1, A_i = v2}\n",
    "                    continue\n",
    "                new_rule_mask = rule_mask & dataset[:,new_cond[0]] == new_cond[1]\n",
    "                if np.sum(new_rule_mask) > min_significant:  # checking significance\n",
    "                    new_rule_entropy = get_entropy(new_rule_mask,labels)\n",
    "                    new_rule = deepcopy(rule_set)\n",
    "                    new_rule.add(new_cond)\n",
    "                    new_rule = fz(new_rule)\n",
    "                    next_candidates[new_rule] = (new_rule_entropy, new_rule_mask) # map takes care of dupelicates\n",
    "                    if(new_rule_entropy < min_entropy): # is this right? can entropy decrease later? yes hence beam search. corrected.\n",
    "                        min_entropy = new_rule_entropy\n",
    "                        best_rule_set = new_rule\n",
    "                        best_rule_mask = new_rule_mask\n",
    "                        # add to next candidates, check for best\n",
    "        att_tosort = {k:dum[0] for k,dum in next_candidates.items()}\n",
    "        sort_next_candidates = [ k for k in sorted(att_tosort, key=att_tosort.get)]\n",
    "        sort_next_candidates = sort_next_candidates[:max_rules_count]\n",
    "        candidates= {}\n",
    "        for k in sort_next_candidates:\n",
    "            candidates[k] = next_candidates[k]\n",
    "    if min_entropy < old_entopy:\n",
    "        return best_rule_set, best_rule_mask\n",
    "    else:\n",
    "#         print(min_entropy, old_entopy)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CN2_train(dataset,labels,all_conds):\n",
    "    # ordered rules is followed\n",
    "    rule_list = []\n",
    "    dataset = deepcopy(dataset)\n",
    "    labels = deepcopy(labels)\n",
    "    dataset_majority_class = Counter(labels.reshape(-1)).most_common(1)[0][0]\n",
    "    \n",
    "    while(len(labels.reshape(-1)) != 0):\n",
    "        next_rule_set = getBestRuleCN2(all_conds, dataset, labels)\n",
    "        if next_rule_set is None:\n",
    "            break\n",
    "        \n",
    "        to_delete = next_rule_set[1]\n",
    "        to_keep = (to_delete == False)\n",
    "        delete_labels = labels[to_delete]\n",
    "        majority_class = Counter(delete_labels.reshape(-1)).most_common(1)[0][0]\n",
    "        rule_list.append((next_rule_set[0],majority_class))\n",
    "        dataset = dataset[to_keep]\n",
    "        labels = labels[to_keep]\n",
    "#         print(labels.shape)\n",
    "    return rule_list, dataset_majority_class\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_to_check_tuples(rule_list):\n",
    "    to_check_tuples = []\n",
    "    for rule,maj_class in rule_list:\n",
    "        attr_list = np.asarray([d[0] for d in rule], dtype=np.int32)\n",
    "        val_list = np.asarray([d[1] for d in rule])\n",
    "        to_check_tuples.append( (attr_list, val_list, maj_class))\n",
    "    return to_check_tuples\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_infer(dataset,rule_list, default_class = -1):\n",
    "    labels = np.zeros((dataset.shape[0],), dtype = np.int32)\n",
    "    labels += default_class;\n",
    "    to_check_tuples = get_to_check_tuples(rule_list)\n",
    "    for idx, sample in enumerate(dataset):\n",
    "        for attr, val, maj_cls in to_check_tuples:\n",
    "            if np.all(sample[attr] == val):\n",
    "                labels[idx] = maj_cls\n",
    "                break;\n",
    "    return labels\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimum description length derived from http://www.csee.usf.edu/~lohall/dm/ripper.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foil_info_gain(p_new, n_new, p_old, n_old):\n",
    "    gain = np.log2(p_new / (p_new+n_new)) - np.log2(p_old / (p_old+n_old))\n",
    "    gain = p_new*gain # acc to slides, multiply by t : no of positive by both old and new rule = p1 bcoz subset superset property\n",
    "    return gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_dl(rule_len, max_rules, p_count, n_count, fp_count, fn_count):\n",
    "    l_model = 0\n",
    "    if rule_len != 0:\n",
    "        l_model = rule_len * np.log2(max_rules/ rule_len) + (max_rules - rule_len)* np.log2(max_rules / (max_rules- rule_len))\n",
    "        l_model += np.log2(rule_len) # encode rule\n",
    "        l_model /= 2 # redundancy removal\n",
    "    \n",
    "    n_comb = np.asarray([p_count, n_count])\n",
    "    k_comb = np.asarray([fp_count, fn_count])\n",
    "    l_error = np.sum(np.log2(comb(n_comb, k_comb)))\n",
    "    \n",
    "    return l_model + l_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_prune_val(p, n):\n",
    "    return (p-n)/(p+n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask_ignore(ruleset, data, ignore_cond = (-1,-1)):\n",
    "    pos_mask = np.asarray([True]*data.shape[0], dtype = np.bool)\n",
    "    for cond in ruleset:\n",
    "        if cond == ignore_cond:\n",
    "            continue\n",
    "        pos_mask = pos_mask & (data[:,cond[0]] == cond[1])\n",
    "    return np.asarray(pos_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rule_Ripper(dataset, labels, pos_class, all_cond):\n",
    "    pos_idx = np.where(labels == pos_class)[0]\n",
    "    neg_idx = np.where(labels != pos_class)[0]\n",
    "    all_pos, all_neg = dataset[pos_idx], dataset[neg_idx]\n",
    "    \n",
    "    grow_pos, prune_pos = train_test_split(all_pos, test_size=0.33, random_state=42)\n",
    "    grow_neg, prune_neg = train_test_split(all_neg, test_size=0.33, random_state=42)\n",
    "    \n",
    "    rule_set = set()\n",
    "    rule_attr = set()\n",
    "    \n",
    "    # growing step\n",
    "    \n",
    "    while grow_neg.shape[0] > 0: # do when thre are negative samples\n",
    "        best_cond = None\n",
    "        best_gain = -1* float('inf')\n",
    "        for new_cond in all_cond:\n",
    "            if new_cond[0] in rule_attr:\n",
    "                continue\n",
    "            p_new = np.sum(grow_pos[:,new_cond[0]] == new_cond[1])\n",
    "            n_new = np.sum(grow_neg[:,new_cond[0]] == new_cond[1])\n",
    "            \n",
    "            new_gain = foil_info_gain(p_new, n_new, grow_pos.shape[0], grow_neg.shape[0])\n",
    "            if new_gain > best_gain:\n",
    "                best_gain = new_gain\n",
    "                best_cond = new_cond\n",
    "            \n",
    "        if best_gain < 0:\n",
    "            break\n",
    "        rule_set.add(best_cond)\n",
    "        rule_attr.add(best_cond[0])\n",
    "        new_grow_pos_mask = grow_pos[:,best_cond[0]] == best_cond[1]\n",
    "        new_grow_neg_mask = grow_neg[:,best_cond[0]] == best_cond[1]\n",
    "        grow_pos = grow_pos[new_grow_pos_mask]\n",
    "        grow_neg = grow_neg[new_grow_neg_mask]\n",
    "        \n",
    "    if len(rule_set) == 0:\n",
    "        return None\n",
    "    # print('rule_len_b4prune:  ',len(rule_set))\n",
    "    # pruning step\n",
    "    pos_rule_mask = get_mask_ignore(rule_set,prune_pos,(-1,-1))\n",
    "    neg_rule_mask = get_mask_ignore(rule_set,prune_neg,(-1,-1))\n",
    "    rule_v = calculate_prune_val(np.sum(pos_rule_mask), np.sum(neg_rule_mask))\n",
    "    # print('rule_val', rule_v, np.sum(pos_rule_mask), np.sum(neg_rule_mask))\n",
    "    while len(rule_set) != 0:\n",
    "        worst_cond = None\n",
    "        max_new_val = -1*float('inf')\n",
    "        for cond in rule_set:\n",
    "            new_pos_p = np.sum(get_mask_ignore(rule_set,prune_pos,cond))\n",
    "            new_neg_n = np.sum(get_mask_ignore(rule_set,prune_neg,cond))\n",
    "            new_rule_v = calculate_prune_val(new_pos_p, new_neg_n)\n",
    "            if new_rule_v > max_new_val:\n",
    "                max_new_val = new_rule_v\n",
    "                worst_cond = cond\n",
    "        if max_new_val >= rule_v:\n",
    "            rule_v = max_new_val\n",
    "            rule_set.remove(worst_cond)\n",
    "            rule_attr.remove(worst_cond[0])\n",
    "            # print('removing', worst_cond)\n",
    "        else: # ie reduce till you can increase the value v.\n",
    "            # print(max_new_val, worst_cond, rule_v)\n",
    "            break\n",
    "    \n",
    "    if len(rule_set) == 0:\n",
    "        return None\n",
    "    # todo error rate:\n",
    "    pos_mask = get_mask_ignore(rule_set,prune_pos,(-1,-1))\n",
    "    neg_mask = get_mask_ignore(rule_set,prune_neg,(-1,-1))\n",
    "    neg_mask = (neg_mask==False)\n",
    "    acc = np.sum(pos_mask) \n",
    "    acc += np.sum(neg_mask)\n",
    "    acc /= (len(pos_mask) + len(neg_mask))\n",
    "    error_rate = 1-acc\n",
    "    if error_rate > 0.5:\n",
    "        # print(error_rate)\n",
    "        return None\n",
    "    \n",
    "    return rule_set, get_mask_ignore(rule_set, dataset, (-1,-1)) \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coverage(nc,n):\n",
    "    return nc/n\n",
    "\n",
    "def accuracy(ncorr,n):\n",
    "    return ncorr/n\n",
    "\n",
    "def laplace(nc,n,k=2):\n",
    "    return (nc+1)/(n+k)\n",
    "\n",
    "def m_estimate(nc, n,p = 0.5,k =2):\n",
    "    return (nc+ (k*p))/(n+k)\n",
    "\n",
    "def all_metrics(nc,n,ncorr,k=2):\n",
    "    return [accuracy(nc,n), coverage(ncorr,n), laplace(nc,n,k)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ripper_train(dataset,labels,all_conds):\n",
    "    # ordered rule is followed for each class\n",
    "    # move on to the next class only after the current minority class is exhausted.\n",
    "    #default class is the majority class\n",
    "    \n",
    "    rule_list = []\n",
    "    dataset = deepcopy(dataset)\n",
    "    labels = deepcopy(labels)\n",
    "    class_freq = Counter(labels.reshape(-1)).most_common()\n",
    "    majority_class = class_freq[0]\n",
    "    class_freq = class_freq[1:]\n",
    "    class_freq.reverse()\n",
    "    \n",
    "    for pos_class, _ in class_freq:\n",
    "        while np.any(labels.reshape(-1) == pos_class):\n",
    "            if len(dataset)==0:\n",
    "                break\n",
    "            next_rule = get_rule_Ripper(dataset, labels, pos_class, all_cond)\n",
    "#             print(next_rule)\n",
    "            if next_rule is None:\n",
    "                break\n",
    "            to_delete = next_rule[1]\n",
    "            # print(np.sum(to_delete))\n",
    "            to_keep = (to_delete == False)\n",
    "            delete_labels = labels[to_delete]\n",
    "            rule_list.append((next_rule[0], pos_class))\n",
    "            # print(rule_list[-1])\n",
    "            dataset = dataset[to_keep]\n",
    "            labels = labels[to_keep]\n",
    "            \n",
    "    return rule_list, majority_class[0]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PassengerId' 'Survived' 'Pclass' 'Name' 'Sex' 'Age' 'SibSp' 'Parch'\n",
      " 'Ticket' 'Fare' 'Cabin' 'Embarked']\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "\n",
    "print(train_df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Has_Cabin'] = train_df[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n",
    "train_df['Embarked'] = train_df['Embarked'].fillna('S')\n",
    "# train_df = train_df.drop(['PassengerId','Name', 'Age', 'Ticket', 'Fare', 'Cabin'], axis=1)\n",
    "train_df['Fare'] = train_df['Fare'].fillna(train_df['Fare'].median())\n",
    "train_df['CategoricalFare'] = pd.qcut(train_df['Fare'], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shikhar/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "age_avg = train_df['Age'].mean()\n",
    "age_std = train_df['Age'].std()\n",
    "age_null_count = train_df['Age'].isnull().sum()\n",
    "age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\n",
    "train_df['Age'][np.isnan(train_df['Age'])] = age_null_random_list\n",
    "train_df['Age'] = train_df['Age'].astype(int)\n",
    "train_df['CategoricalAge'] = pd.cut(train_df['Age'], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in [train_df]:\n",
    "    # Mapping Sex\n",
    "    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n",
    "    \n",
    "    # Mapping Embarked\n",
    "    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n",
    "    \n",
    "    # Mapping Fare\n",
    "    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n",
    "    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n",
    "    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare'] = 2\n",
    "    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n",
    "    dataset['Fare'] = dataset['Fare'].astype(int)\n",
    "    \n",
    "    # Mapping Age\n",
    "    dataset.loc[ dataset['Age'] <= 16, 'Age'] \t\t\t\t\t       = 0\n",
    "    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n",
    "    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n",
    "    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n",
    "    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4 ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp']\n",
    "train_df = train_df.drop(drop_elements, axis = 1)\n",
    "train_df = train_df.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Survived' 'Pclass' 'Sex' 'Age' 'Parch' 'Fare' 'Embarked' 'Has_Cabin']\n"
     ]
    }
   ],
   "source": [
    "print(train_df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2attr = {i: attribute for i, attribute in enumerate(train_df.columns.values[1:])}\n",
    "\n",
    "condition2string = {(0, 1): 'Pclass is 1', (0, 2): 'Pclass is 2', (0, 3): 'Pclass is 3',\n",
    "                    (1, 0): 'Sex is Female', (1, 1): 'Sex is Male',\n",
    "                    (2, 0): 'Age <= 16', (2, 1): '16 < Age <= 32', (2, 2): '32 < Age <= 48', (2, 3): '48 < Age <= 64', (2, 4): 'Age > 64',\n",
    "                    (3, 0): '#Parent/Child is 0', (3, 1): '#Parent/Child is 1', (3, 2): '#Parent/Child is 2', (3, 3): '#Parent/Child is 3', (3, 4): '#Parent/Child is 4', (3, 5): '#Parent/Child is 5', (3, 6): '#Parent/Child is 6',\n",
    "                    (4, 0): 'Fare <= 7.91', (4, 1): '7.91 < Fare <= 14.454', (4, 2): '14.454 < Fare <= 31.0', (4, 3): 'Fare > 31.0',\n",
    "                    (5, 0): 'Embarked from Southampton', (5, 1): 'Embarked from Cherbourg', (5, 2): 'Embarked from Queenstown',\n",
    "                    (6, 1): 'Has a cabin', (6, 0): 'Doesn\\'t have a cabin'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfn = train_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = tfn[:,1:], tfn[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "poss_val_dict = dict()\n",
    "for i in range(x.shape[1]):\n",
    "    poss_val = list(set(x[:,i]))\n",
    "    poss_val_dict[i] = poss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (0, 2), (0, 3), (1, 0), (1, 1), (2, 0), (2, 1), (2, 2), (2, 3), (2, 4), (3, 0), (3, 1), (3, 2), (3, 3), (3, 4), (3, 5), (3, 6), (4, 0), (4, 1), (4, 2), (4, 3), (5, 0), (5, 1), (5, 2), (6, 0), (6, 1)]\n"
     ]
    }
   ],
   "source": [
    "all_cond = []\n",
    "for lhs, all_rhs in poss_val_dict.items():\n",
    "    dum = [(lhs, b) for b in all_rhs]\n",
    "    all_cond = all_cond + dum\n",
    "\n",
    "print(all_cond)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr, x_te,y_tr,y_te = train_test_split(x,y, test_size=0.2, random_state= 96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((712, 7), (712,))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tr.shape , y_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "out = CN2_train(x_tr,y_tr,deepcopy(all_cond))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = rule_infer(x_te,out[0], default_class = out[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of CN2: 0.7933\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of CN2: %0.4f'%(np.sum(y_hat == y_te) / len(y_hat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_p = {}\n",
    "prior_p[0] = (np.sum(y_tr == 0) / len(y_tr))\n",
    "prior_p[1] = 1 - prior_p[0]\n",
    "k = 2\n",
    "ruleset_metrics = [] # (no_of_rules, 6)\n",
    "for rule in out[0]:\n",
    "    rule_metrics = [] # (6,) => (3,) train (3,) test\n",
    "    for data in [(x_tr,y_tr), (x_te,y_te)]:\n",
    "        rule_mask = get_mask_ignore(rule[0], data[0])\n",
    "        nc = np.sum(rule_mask)\n",
    "        n = len(rule_mask)\n",
    "        y_hat = rule_infer(data[0],[rule], default_class = out[1])\n",
    "        ncorr = np.sum(y_hat == data[1])\n",
    "        rule_metrics += all_metrics(nc,n,ncorr,k)\n",
    "    rule_metrics = np.asarray(rule_metrics, dtype=np.float32)\n",
    "    ruleset_metrics.append(rule_metrics)\n",
    "ruleset_metrics = np.asarray(ruleset_metrics, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IF ( Sex is Female ) THEN Survived\n",
      "Accuracy:0.296 Coverage: 0.793, laplace: 0.298\n",
      "\n",
      "IF ( Doesn't have a cabin ) THEN Didn't Survive\n",
      "Accuracy:0.771 Coverage: 0.687, laplace: 0.768\n",
      "\n",
      "IF ( Pclass is 1 ) AND ( #Parent/Child is 0 ) AND ( Sex is Female ) THEN Didn't Survive\n",
      "Accuracy:0.045 Coverage: 0.687, laplace: 0.050\n",
      "\n",
      "ELSE: Didn't Survive\n"
     ]
    }
   ],
   "source": [
    "id2label = {0: 'Didn\\'t Survive',\n",
    "            1: 'Survived'}\n",
    "\n",
    "for ruleno, rule in enumerate(out[0]):\n",
    "    to_print = []\n",
    "    \n",
    "    for cond in rule[0]:\n",
    "        to_print.append('( ' + condition2string[cond] + ' )')\n",
    "    to_print = [\"IF \", \" AND \".join(to_print)]\n",
    "    to_print.append(\" THEN {}\".format(id2label[rule[1]]))\n",
    "    print(''.join(to_print))\n",
    "    print('Accuracy:%.3f Coverage: %.3f, laplace: %.3f\\n'%(ruleset_metrics[ruleno][3], ruleset_metrics[ruleno][4], ruleset_metrics[ruleno][5]))\n",
    "print('ELSE: {}'.format(id2label[out[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shikhar/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in log2\n",
      "  \n",
      "/home/shikhar/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in multiply\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/shikhar/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "out = ripper_train(x_tr,y_tr,deepcopy(all_cond))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = rule_infer(x_te,out[0], default_class = out[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Ripper: 0.8156\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of Ripper: %0.4f'%(np.sum(y_hat == y_te) / len(y_hat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_p = {}\n",
    "prior_p[0] = (np.sum(y_tr == 0) / len(y_tr))\n",
    "prior_p[1] = 1 - prior_p[0]\n",
    "k = 2\n",
    "ruleset_metrics = [] # (no_of_rules, 6)\n",
    "for rule in out[0]:\n",
    "    rule_metrics = [] # (6,) => (3,) train (3,) test\n",
    "    for data in [(x_tr,y_tr), (x_te,y_te)]:\n",
    "        rule_mask = get_mask_ignore(rule[0], data[0])\n",
    "        nc = np.sum(rule_mask)\n",
    "        n = len(rule_mask)\n",
    "        y_hat = rule_infer(data[0],[rule], default_class = out[1])\n",
    "        ncorr = np.sum(y_hat == data[1])\n",
    "        rule_metrics += all_metrics(nc,n,ncorr,k)\n",
    "    rule_metrics = np.asarray(rule_metrics, dtype=np.float32)\n",
    "    ruleset_metrics.append(rule_metrics)\n",
    "ruleset_metrics = np.asarray(ruleset_metrics, dtype=np.float32)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IF ( Pclass is 1 ) AND ( Sex is Female ) THEN Survived\n",
      "Accuracy:0.067 Coverage: 0.732, laplace: 0.072\n",
      "\n",
      "IF ( Sex is Female ) AND ( Pclass is 2 ) THEN Survived\n",
      "Accuracy:0.073 Coverage: 0.737, laplace: 0.077\n",
      "\n",
      "IF ( 14.454 < Fare <= 31.0 ) AND ( Sex is Female ) AND ( Embarked from Queenstown ) THEN Survived\n",
      "Accuracy:0.006 Coverage: 0.682, laplace: 0.011\n",
      "\n",
      "IF ( Sex is Female ) AND ( Fare <= 7.91 ) THEN Survived\n",
      "Accuracy:0.050 Coverage: 0.715, laplace: 0.055\n",
      "\n",
      "IF ( Age <= 16 ) AND ( Has a cabin ) THEN Survived\n",
      "Accuracy:0.022 Coverage: 0.698, laplace: 0.028\n",
      "\n",
      "ELSE: Didn't Survive\n"
     ]
    }
   ],
   "source": [
    "id2label = {0: 'Didn\\'t Survive',\n",
    "            1: 'Survived'}\n",
    "\n",
    "for ruleno, rule in enumerate(out[0]):\n",
    "    to_print = []\n",
    "    \n",
    "    for cond in rule[0]:\n",
    "        to_print.append('( ' + condition2string[cond] + ' )')\n",
    "    to_print = [\"IF \", \" AND \".join(to_print)]\n",
    "    to_print.append(\" THEN {}\".format(id2label[rule[1]]))\n",
    "    print(''.join(to_print))\n",
    "    print('Accuracy:%.3f Coverage: %.3f, laplace: %.3f\\n'%(ruleset_metrics[ruleno][3], ruleset_metrics[ruleno][4], ruleset_metrics[ruleno][5]))\n",
    "print('ELSE: {}'.format(id2label[out[1]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
