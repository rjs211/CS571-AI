{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Import the required libraries.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re, random\n",
    "from copy import copy, deepcopy\n",
    "import sys\n",
    "from collections import Counter\n",
    "import queue\n",
    "from scipy import stats\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "np.random.seed(21)\n",
    "random.seed(21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('tagsets')\n",
    "# from nltk.corpus import stopwords\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import FreqDist\n",
    "from nltk import ngrams\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_text_labe(data):\n",
    "    text = []\n",
    "    answer_type = []\n",
    "    label = []\n",
    "    sent_length = []\n",
    "    for line in data:\n",
    "        a = line.split(':', maxsplit=1)\n",
    "        label.append(a[0])\n",
    "        b = a[1].strip().split(' ',maxsplit=1)\n",
    "        text.append(b[1].lower())\n",
    "        answer_type.append(b[0])\n",
    "    # remove punctuations\n",
    "    clean_text = [re.sub(r'([^\\w\\s]|[0-9])', ' ', line) for line in text]\n",
    "    clean_text = [re.sub(r'(\\s+)', ' ', line) for line in clean_text]\n",
    "    return clean_text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.add('')\n",
    "    text_tokens = [sent.split() for sent in text]\n",
    "    text_no_stopwords = [[w for w in words if w not in stop_words] for words in text_tokens]\n",
    "    return text_no_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def ngram_topk(token_list, n, k, feat_dicts=None):\n",
    "    ngrams_list = [list(ngrams(sent, n)) for sent in token_list]\n",
    "    if feat_dicts:\n",
    "        ngrams_topk_dict, idx2ngram_dict = feat_dicts\n",
    "    else:\n",
    "        all_ngrams = sum(ngrams_list, [])\n",
    "        freq_dist = FreqDist(all_ngrams)\n",
    "        freq_dist_k = freq_dist.most_common(k)\n",
    "        ngrams_topk_list =  [ngram_token for ngram_token, _ in freq_dist_k]\n",
    "        ngrams_topk_dict = {ngram_token:i for i, ngram_token in enumerate(ngrams_topk_list)}\n",
    "        idx2ngram_dict = {v: k for k, v in ngrams_topk_dict.items()}\n",
    "    \n",
    "    ngrams_freq_feat = []\n",
    "    for ngram_tokens in ngrams_list:\n",
    "        ngram_token_freq = np.zeros(k, dtype = np.int32)\n",
    "        for ngram_token in ngram_tokens:\n",
    "            if ngram_token in ngrams_topk_dict.keys():\n",
    "                ngram_token_freq[ ngrams_topk_dict[ngram_token] ]+=1\n",
    "        ngrams_freq_feat.append(ngram_token_freq)\n",
    "    \n",
    "    return np.asarray(ngrams_freq_feat, dtype = np.int32), ngrams_topk_dict, idx2ngram_dict, ngrams_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read the file line by line and clean the text of punctuation.\n",
    "with open('Data/train_5500.label', 'r',encoding='latin-1') as file:\n",
    "    data = file.readlines()\n",
    "\n",
    "X_train, Y_train = get_text_labe(data)\n",
    "\n",
    "with open('./Data/TREC_10.label', 'r',encoding='latin-1') as file:\n",
    "    data = file.readlines()\n",
    "\n",
    "X_test, Y_test = get_text_labe(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_features_train(X_train, feats_to_use = ('len', 'uni' ,'bi' ,'tri' ,'pos')):\n",
    "    X_train = [sent.split() for sent in X_train]\n",
    "    features = []\n",
    "    feat_dicts_list = []\n",
    "\n",
    "    if 'len' in feats_to_use:\n",
    "        X_train_sentlen = np.reshape(np.asarray([len(sent) for sent in X_train], dtype = np.int32),(-1,1))\n",
    "        feat_dicts_list.append(None)\n",
    "    else:\n",
    "        X_train_sentlen = np.reshape(np.asarray([-1 for sent in X_train], dtype = np.int32),(-1,1))\n",
    "    features.append(X_train_sentlen)\n",
    "    \n",
    "    if 'uni' in feats_to_use:\n",
    "        X_train_unigram = ngram_topk(X_train, 1, 500)\n",
    "        features.append(X_train_unigram[0])\n",
    "        feat_dicts_list.append(X_train_unigram[1:-1])\n",
    "    if 'bi' in feats_to_use:\n",
    "        X_train_bigram = ngram_topk(X_train, 2, 300)\n",
    "        features.append(X_train_bigram[0])\n",
    "        feat_dicts_list.append(X_train_bigram[1:-1])\n",
    "    if 'tri' in feats_to_use:\n",
    "        X_train_trigram = ngram_topk(X_train, 3, 200)\n",
    "        features.append(X_train_trigram[0])\n",
    "        feat_dicts_list.append(X_train_trigram[1:-1])\n",
    "    if 'pos' in feats_to_use:\n",
    "        X_train_pos = [pos_tag(tokens) for tokens in X_train]\n",
    "        X_train_pos_unigram = ngram_topk(X_train_pos, 1, 500)\n",
    "        features.append(X_train_pos_unigram[0])\n",
    "        feat_dicts_list.append(X_train_pos_unigram[1:-1])\n",
    "\n",
    "    X_train_feats = np.concatenate( features, axis=1 )\n",
    "    \n",
    "    return X_train_feats, feat_dicts_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_features_test(X_test, feat_dicts_list, feats_to_use = ('len', 'uni' ,'bi' ,'tri' ,'pos')):\n",
    "    X_test = [sent.split() for sent in X_test]\n",
    "    features = []\n",
    "    feat_dicts_list_idx = 0\n",
    "    assert len(feat_dicts_list) == len(feats_to_use)\n",
    "    if 'len' in feats_to_use:\n",
    "        X_test_sentlen = np.reshape(np.asarray([len(sent) for sent in X_test], dtype = np.int32),(-1,1))\n",
    "        feat_dicts_list_idx += 1\n",
    "    else:\n",
    "        X_test_sentlen = np.reshape(np.asarray([-1 for sent in X_test], dtype = np.int32),(-1,1))\n",
    "    features.append(X_test_sentlen)\n",
    "    \n",
    "    if 'uni' in feats_to_use:\n",
    "        X_test_unigram = ngram_topk(X_test, 1, 500, feat_dicts_list[feat_dicts_list_idx])\n",
    "        features.append(X_test_unigram[0])\n",
    "        feat_dicts_list_idx += 1\n",
    "    if 'bi' in feats_to_use:\n",
    "        X_test_bigram = ngram_topk(X_test, 2, 300, feat_dicts_list[feat_dicts_list_idx])\n",
    "        features.append(X_test_bigram[0])\n",
    "        feat_dicts_list_idx += 1\n",
    "    if 'tri' in feats_to_use:\n",
    "        X_test_trigram = ngram_topk(X_test, 3, 200, feat_dicts_list[feat_dicts_list_idx])\n",
    "        features.append(X_test_trigram[0])\n",
    "        feat_dicts_list_idx += 1\n",
    "    if 'pos' in feats_to_use:\n",
    "        X_test_pos = [pos_tag(tokens) for tokens in X_test]\n",
    "        X_test_pos_unigram = ngram_topk(X_test_pos, 1, 500, feat_dicts_list[feat_dicts_list_idx])\n",
    "        features.append(X_test_pos_unigram[0])\n",
    "        feat_dicts_list_idx += 1\n",
    "    \n",
    "    assert len(feat_dicts_list) == feat_dicts_list_idx\n",
    "    X_test_feats = np.concatenate( features, axis=1 )\n",
    "    return X_test_feats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_feats, feat_dicts_list = get_features_train(X_train, feats_to_use = ('len', 'uni' ,'bi' ,'tri' ,'pos'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_test_feats = get_features_test(X_test, feat_dicts_list, feats_to_use = ('len', 'uni' ,'bi' ,'tri' ,'pos'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "label2idx = {lab: i for i, lab in enumerate(set(Y_train))}\n",
    "idx2label = {i: lab for lab, i in label2idx.items()}\n",
    "print(label2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "Y_train_idx = np.asarray([label2idx[lab] for lab in Y_train], dtype=np.int32)\n",
    "Y_test_idx = np.asarray([label2idx[lab] for lab in Y_test], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Node():\n",
    "    cnt = 0\n",
    "    def __init__(self, ):\n",
    "        self.leaf = False\n",
    "        self.majority_class = None\n",
    "        self.attribute_index = None\n",
    "        self.children = dict() # key: attribute_value, value: child_node\n",
    "        self.sent_len_split_val = None # Used at inference time, if attribute_index is 0\n",
    "        self.id = Node.cnt\n",
    "        Node.cnt+=1\n",
    "    \n",
    "    def __str__(self,):\n",
    "        return 'ID: {}  isLeaf: {} majority: {} split_idx: {} split_val = {}'.format(self.id, \n",
    "                                                                                    self.leaf, \n",
    "                                                                                    self.majority_class, \n",
    "                                                                                    self.attribute_index, \n",
    "                                                                                    list(self.children.keys())\n",
    "                                                                                   )\n",
    "    def __repr__(self):\n",
    "        return str(self)\n",
    "    \n",
    "    def traverse_print(self,):\n",
    "        print(self)\n",
    "        for _, child in self.children:\n",
    "              child.traverse_print()\n",
    "\n",
    "    @classmethod\n",
    "    def reset_cnt(cls,):\n",
    "        cls.cnt = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "    \n",
    "    # score has to be from 'entropy', 'gini', 'misclassification'\n",
    "    def __init__(self, score='entropy'):\n",
    "        score_functions = {'entropy': (DecisionTree.compute_entropy, DecisionTree.get_gain_entropy),\n",
    "           'gini': (DecisionTree.compute_gini, DecisionTree.get_gain_gini),\n",
    "           'misclassification': (DecisionTree.compute_misclassification, DecisionTree.get_gain_misclassification)}\n",
    "        \n",
    "        self.root = None\n",
    "        assert score in score_functions.keys()\n",
    "        self.score = score\n",
    "        self.compute_score = score_functions[score][0]\n",
    "        self.get_gain = score_functions[score][1]\n",
    "        return\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_entropy(labels):\n",
    "        entropy = 0.0\n",
    "        totSamples = len(labels)\n",
    "        labelSet = set(labels.reshape(-1))\n",
    "        for label in labelSet:\n",
    "            prob = np.sum(labels == label) / totSamples\n",
    "            if prob > 1e-12:\n",
    "                entropy -= np.log(prob) * prob\n",
    "        \n",
    "        return entropy\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_gain_entropy(parent_info, data_i, labels):\n",
    "        attr_split_info = 0\n",
    "        attr_count = dict()\n",
    "        for attr_val in set(data_i.reshape(-1)):\n",
    "            ids = np.where(data_i == attr_val)[0]\n",
    "            attr_count[attr_val] = len(ids)\n",
    "            attr_split_info += attr_count[attr_val] * DecisionTree.compute_entropy(labels[ids])\n",
    "        attr_gain = parent_info - attr_split_info\n",
    "        attr_gain_ratio = DecisionTree.compute_dict_entropy(attr_count) * attr_gain\n",
    "        return attr_gain, attr_gain_ratio, attr_count.keys()\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_dict_entropy(attr_count):\n",
    "        entropy = 0\n",
    "        totSamples = sum(attr_count.values())\n",
    "       \n",
    "        labelSet = attr_count.keys()\n",
    "        for label in labelSet:\n",
    "            prob = attr_count[label] / totSamples\n",
    "            if prob > 1e-12:\n",
    "                entropy -= np.log(prob) * prob\n",
    "        return entropy\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_gini(labels):\n",
    "        prob_sq = 0.0\n",
    "        totSamples = len(labels)\n",
    "        labelSet = set(labels.reshape(-1))\n",
    "        for label in labelSet:\n",
    "            prob = np.sum(labels == label) / totSamples\n",
    "            if prob > 1e-12:\n",
    "                prob_sq += prob*prob\n",
    "        return 1-prob_sq\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def get_gain_gini(parent_info, data_i, labels):\n",
    "        attr_split_info = 0\n",
    "        attr_count = dict()\n",
    "        for attr_val in set(data_i.reshape(-1)):\n",
    "            ids = np.where(data_i == attr_val)[0]\n",
    "            attr_count[attr_val] = len(ids)\n",
    "            attr_split_info += attr_count[attr_val] * DecisionTree.compute_gini(labels[ids])\n",
    "        attr_split_info /= data_i.shape[0]\n",
    "        \n",
    "        attr_gain = parent_info - attr_split_info\n",
    "        #attr_gain_ratio = DecisionTree.compute_dict_entropy(attr_count) * attr_gain\n",
    "        return attr_gain, attr_gain, attr_count.keys()\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_misclassification(labels):\n",
    "        max_prob = -1\n",
    "        totSamples = len(labels)\n",
    "        labelSet = set(labels.reshape(-1))\n",
    "        for label in labelSet:\n",
    "            prob = np.sum(labels == label) / totSamples\n",
    "            if prob > max_prob:\n",
    "                max_prob = prob\n",
    "        \n",
    "        return 1-max_prob\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_gain_misclassification(parent_info, data_i, labels):\n",
    "        attr_split_info = -1\n",
    "        attr_count = dict()\n",
    "        for attr_val in set(data_i.reshape(-1)):\n",
    "            ids = np.where(data_i == attr_val)[0]\n",
    "            attr_count[attr_val] = len(ids)\n",
    "            attr_split_info = attr_count[attr_val] * DecisionTree.compute_misclassification(labels[ids])\n",
    "        attr_split_info /= data_i.shape[0]\n",
    "        attr_gain = parent_info - attr_split_info\n",
    "        return attr_gain, attr_gain, attr_count.keys()\n",
    "    \n",
    "    def split_node(self, parent, data, labels, used_attr_index):\n",
    "        num_instances = data.shape[0]\n",
    "        parent_info = self.compute_score(labels) * num_instances\n",
    "        parent.majority_class = Counter(labels.reshape(-1)).most_common(1)[0][0]\n",
    "        \n",
    "        if parent_info == 0 :\n",
    "            parent.leaf = True\n",
    "        \n",
    "        best_attr_index = None\n",
    "        best_info_gain = -float('inf')\n",
    "        best_gain_ratio = -float('inf')\n",
    "        best_attr_keys = None\n",
    "#         sent length case special\n",
    "#         attr_split_info = 0\n",
    "#         attr_count = dict()\n",
    "        sent_len_split_val = stats.mode(data[:, 0])[0][0]\n",
    "        le_ids = np.where(data[:, 0] <= sent_len_split_val)[0]\n",
    "        gt_ids = np.where(data[:, 0] > sent_len_split_val)[0]\n",
    "        data_0 = np.zeros(data.shape[0], dtype=np.int32)\n",
    "        data_0[gt_ids] = 1\n",
    "#         attr_count[0] = le_ids.shape[0]\n",
    "#         attr_count[1] = gt_ids.shape[0]\n",
    "#         attr_split_info = (attr_count[0] * self.compute_entropy(labels[le_ids])) + (attr_count[1] * self.compute_entropy(labels[gt_ids]) )    \n",
    "#         attr_gain = parent_info - attr_split_info\n",
    "        attr_gain, attr_gain_ratio, attr_count_keys = self.get_gain(parent_info, data_0, labels)\n",
    "#         attr_gain_ratio = self.compute_dict_entropy(attr_count) * attr_gain\n",
    "        if best_gain_ratio < attr_gain_ratio and  attr_gain_ratio > 0 :\n",
    "                best_attr_index = 0\n",
    "                best_info_gain = attr_gain\n",
    "                best_gain_ratio = attr_gain_ratio\n",
    "                best_attr_keys = attr_count_keys\n",
    "        \n",
    "        # during ablation, sentence length can be initialized to all zeros this will prevent splittiung in sent dimension/.\n",
    "        for i in range(1, data.shape[1]): # starts from 1 as zero is sentence length (always.) .\n",
    "            if i in used_attr_index:\n",
    "                continue\n",
    "            attr_gain, attr_gain_ratio, attr_count_keys = self.get_gain(parent_info, data[:, i], labels)\n",
    "            if best_gain_ratio < attr_gain_ratio:\n",
    "                best_attr_index = i\n",
    "                best_info_gain = attr_gain\n",
    "                best_gain_ratio = attr_gain_ratio\n",
    "                best_attr_keys = attr_count_keys\n",
    "        if best_gain_ratio <= 0 or len(best_attr_keys) == 1 :\n",
    "            parent.leaf = True\n",
    "            return [] # TO Check    \n",
    "        else:\n",
    "            parent.attribute_index =  best_attr_index\n",
    "            parent.children = { i: Node() for i in best_attr_keys}\n",
    "            to_return = []\n",
    "            if best_attr_index != 0:\n",
    "                used_attr_index.append(best_attr_index)\n",
    "                for i in best_attr_keys:\n",
    "                    inds = np.where(data[:, best_attr_index] == i)[0]\n",
    "                    to_return.append( (parent.children[i], data[inds], labels[inds], used_attr_index) )\n",
    "            else:\n",
    "                parent.sent_len_split_val = sent_len_split_val\n",
    "#                 print(len(best_attr_keys))\n",
    "                for i in best_attr_keys:\n",
    "                    inds = np.where(data_0 == i)[0]\n",
    "                    to_return.append( (parent.children[i], data[inds], labels[inds], used_attr_index) )\n",
    "                    \n",
    "#                 to_return.append( (parent.children[0], data[le_ids], labels[le_ids], used_attr_index) )\n",
    "#                 to_return.append( (parent.children[1], data[gt_ids], labels[gt_ids], used_attr_index) )\n",
    "            return to_return\n",
    "    \n",
    "    def build_tree(self, data, labels):\n",
    "        traversal_q = queue.Queue()\n",
    "        root = Node()\n",
    "        self.root = root\n",
    "        traversal_q.put_nowait( (root, data, labels, [] ))\n",
    "        cent = 0\n",
    "        while not traversal_q.empty():\n",
    "            node_to_split = traversal_q.get_nowait()\n",
    "            child_nodes = self.split_node(*node_to_split)\n",
    "            for child in child_nodes:\n",
    "                traversal_q.put_nowait(child)\n",
    "            if Node.cnt/100 > cent:\n",
    "                print(Node.cnt)\n",
    "                cent+=1\n",
    "        return root\n",
    "    \n",
    "    def split_infer(self, node, data, data_indices):\n",
    "        if node.leaf:\n",
    "            return (True, data_indices, np.zeros( (data.shape[0]), dtype = np.int32) + node.majority_class)\n",
    "        else:\n",
    "            to_queue = []\n",
    "            if(node.attribute_index == 0):\n",
    "                left_idx = np.where(data[:,0] <= node.sent_len_split_val)[0]\n",
    "                right_idx = np.where(data[:,0] > node.sent_len_split_val)[0]\n",
    "                to_queue.append( (node.children[0], data[left_idx], data_indices[left_idx]) )\n",
    "                to_queue.append( (node.children[1], data[right_idx], data_indices[right_idx]) )\n",
    "                return (False, to_queue)\n",
    "            else:\n",
    "                for i in node.children.keys():\n",
    "                    split_inds = np.where( data[:, node.attribute_index]  == i)[0]\n",
    "                    if len(split_inds) > 0:\n",
    "                        to_queue.append( (node.children[i], data[split_inds], data_indices[split_inds]) )\n",
    "                return (False, to_queue)\n",
    "    \n",
    "    def split_infer_depth(self, node, data, data_indices,depth):\n",
    "        if node.leaf or depth >=10:\n",
    "            return (True, data_indices, np.zeros( (data.shape[0]), dtype = np.int32) + node.majority_class)\n",
    "        else:\n",
    "            to_queue = []\n",
    "            if(node.attribute_index == 0):\n",
    "                left_idx = np.where(data[:,0] <= node.sent_len_split_val)[0]\n",
    "                right_idx = np.where(data[:,0] > node.sent_len_split_val)[0]\n",
    "                to_queue.append( (node.children[0], data[left_idx], data_indices[left_idx],depth+1) )\n",
    "                to_queue.append( (node.children[1], data[right_idx], data_indices[right_idx], depth+1) )\n",
    "                return (False, to_queue)\n",
    "            else:\n",
    "                for i in node.children.keys():\n",
    "                    split_inds = np.where( data[:, node.attribute_index]  == i)[0]\n",
    "                    if len(split_inds) > 0:\n",
    "                        to_queue.append( (node.children[i], data[split_inds], data_indices[split_inds],depth+1) )\n",
    "                return (False, to_queue)\n",
    "    \n",
    "    def get_labels(self, data):\n",
    "        root = self.root\n",
    "        data_idx = np.arange(data.shape[0], dtype = np.int32)\n",
    "        labels = np.zeros( (data.shape[0]), dtype = np.int32) + -1\n",
    "        traversal_q = queue.Queue()\n",
    "        traversal_q.put_nowait( (root, data, data_idx ))\n",
    "        while not traversal_q.empty():\n",
    "            node_to_split = traversal_q.get_nowait()\n",
    "            split_return = self.split_infer(*node_to_split)\n",
    "            if split_return[0]:\n",
    "                labels[split_return[1]] = split_return[2]\n",
    "            else:\n",
    "                for child in split_return[1]:\n",
    "                    traversal_q.put_nowait(child)\n",
    "        return labels\n",
    "    \n",
    "    def get_labels_depth(self, data):\n",
    "        root = self.root\n",
    "        data_idx = np.arange(data.shape[0], dtype = np.int32)\n",
    "        labels = np.zeros( (data.shape[0]), dtype = np.int32) + -1\n",
    "        traversal_q = queue.Queue()\n",
    "        traversal_q.put_nowait( (root, data, data_idx ,0))\n",
    "        while not traversal_q.empty():\n",
    "            node_to_split = traversal_q.get_nowait()\n",
    "            split_return = self.split_infer_depth(*node_to_split)\n",
    "            if split_return[0]:\n",
    "                labels[split_return[1]] = split_return[2]\n",
    "            else:\n",
    "                for child in split_return[1]:\n",
    "                    traversal_q.put_nowait(child)\n",
    "        return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Binarize data\n",
    "X_train_feats_bin = deepcopy(X_train_feats)\n",
    "X_train_feats_bin[:, 1:] = (X_train_feats[:, 1:] > 0).astype(np.int32)\n",
    "\n",
    "X_test_feats_bin = deepcopy(X_test_feats)\n",
    "X_test_feats_bin[:, 1:] = (X_test_feats[:, 1:] > 0).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dtree = DecisionTree('gini')\n",
    "root = dtree.build_tree(data=X_train_feats_bin, labels=Y_train_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Node.cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dtree = DecisionTree('entropy')\n",
    "root = dtree.build_tree(data=X_train_feats_bin, labels=Y_train_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dtree.root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_pred_test = dtree.get_labels(data=X_test_feats_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_scores(Y_test_idx, y_pred_test):\n",
    "    acc = (y_pred_test == Y_test_idx).mean()\n",
    "    prec, rec, fscore, _ = precision_recall_fscore_support(Y_test_idx, y_pred_test, average='weighted')\n",
    "    return acc, prec, rec, fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print('Acc: {}, Prec: {}, Rec: {}, Fscore: {}'.format(*get_scores(Y_test_idx, y_pred_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "precision_recall_fscore_support(Y_test_idx, y_pred_test, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "all_features = {'len', 'uni' ,'bi' ,'tri' ,'pos'}\n",
    "dtree_list = dict()\n",
    "scores_list = dict()\n",
    "\n",
    "for feat_to_drop in all_features:\n",
    "    feats_to_use = frozenset(all_features - {feat_to_drop})\n",
    "    X_train_feats, feat_dicts_list = get_features_train(X_train, feats_to_use = feats_to_use)\n",
    "    X_test_feats = get_features_test(X_test, feat_dicts_list, feats_to_use = feats_to_use)\n",
    "    X_train_feats_bin = deepcopy(X_train_feats)\n",
    "    X_train_feats_bin[:, 1:] = (X_train_feats[:, 1:] > 0).astype(np.int32)\n",
    "    \n",
    "    X_test_feats_bin = deepcopy(X_test_feats)\n",
    "    X_test_feats_bin[:, 1:] = (X_test_feats[:, 1:] > 0).astype(np.int32)\n",
    "    dtree = DecisionTree()\n",
    "    _ = dtree.build_tree(data=X_train_feats_bin, labels=Y_train_idx)\n",
    "    dtree_list[feats_to_use] = dtree\n",
    "    y_pred_test = dtree.get_labels(data=X_test_feats_bin)\n",
    "    all_scores = get_scores(Y_test_idx, y_pred_test)\n",
    "    scores_list[feats_to_use] = all_scores\n",
    "    print('Features: {}, Missing Feature: {}'.format(feats_to_use, feat_to_drop))\n",
    "    print('Acc: {}, Prec: {}, Rec: {}, Fscore: {}'.format(*all_scores))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_features = {'len', 'uni' ,'bi' ,'tri' ,'pos'}\n",
    "\n",
    "feats_to_use = frozenset(all_features - {'uni', 'bi', 'tri'})\n",
    "X_train_feats, feat_dicts_list = get_features_train(X_train, feats_to_use = feats_to_use)\n",
    "X_test_feats = get_features_test(X_test, feat_dicts_list, feats_to_use = feats_to_use)\n",
    "X_train_feats_bin = deepcopy(X_train_feats)\n",
    "X_train_feats_bin[:, 1:] = (X_train_feats[:, 1:] > 0).astype(np.int32)\n",
    "\n",
    "X_test_feats_bin = deepcopy(X_test_feats)\n",
    "X_test_feats_bin[:, 1:] = (X_test_feats[:, 1:] > 0).astype(np.int32)\n",
    "dtree = DecisionTree()\n",
    "_ = dtree.build_tree(data=X_train_feats_bin, labels=Y_train_idx)\n",
    "dtree_list[feats_to_use] = dtree\n",
    "y_pred_test = dtree.get_labels(data=X_test_feats_bin)\n",
    "all_scores = get_scores(Y_test_idx, y_pred_test)\n",
    "scores_list[feats_to_use] = all_scores\n",
    "print('Features: {}, Missing Feature: {}'.format(feats_to_use, feat_to_drop))\n",
    "print('Acc: {}, Prec: {}, Rec: {}, Fscore: {}'.format(*all_scores))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "all_features = {'len', 'uni' ,'bi' ,'tri' ,'pos'}\n",
    "dtree_list = dict()\n",
    "scores_dict = dict()\n",
    "preds_dict = dict()\n",
    "gain_types = ['entropy', 'gini', 'misclassification']\n",
    "\n",
    "for gain_type in gain_types:\n",
    "    X_train_feats, feat_dicts_list = get_features_train(X_train, feats_to_use = all_features)\n",
    "    X_test_feats = get_features_test(X_test, feat_dicts_list, feats_to_use = all_features)\n",
    "    X_train_feats_bin = deepcopy(X_train_feats)\n",
    "    X_train_feats_bin[:, 1:] = (X_train_feats[:, 1:] > 0).astype(np.int32)\n",
    "    X_test_feats_bin = deepcopy(X_test_feats)\n",
    "    X_test_feats_bin[:, 1:] = (X_test_feats[:, 1:] > 0).astype(np.int32)\n",
    "    dtree = DecisionTree(score=gain_type)\n",
    "    _ = dtree.build_tree(data=X_train_feats_bin, labels=Y_train_idx)\n",
    "    dtree_list[gain_type] = dtree\n",
    "    y_pred_test = dtree.get_labels(data=X_test_feats_bin)\n",
    "    all_scores = get_scores(Y_test_idx, y_pred_test)\n",
    "    scores_dict[gain_type] = all_scores\n",
    "    preds_dict[gain_type] = y_pred_test\n",
    "    print('Gain type: {}'.format(gain_type))\n",
    "    print('Acc: {}, Prec: {}, Rec: {}, Fscore: {}'.format(*all_scores))\n",
    "    print()\n",
    "\n",
    "# TODO: Compute per-class accuracies, and print the best model types for each class\n",
    "for label in label2idx.keys():\n",
    "    idx = label2idx[label]\n",
    "    label_mask = (Y_test_idx == idx)\n",
    "    y_true = Y_test_idx[label_mask]\n",
    "    best_gain_type = None\n",
    "    best_acc = -float('inf')\n",
    "    for gain_type in gain_types:\n",
    "        y_pred = preds_dict[gain_type][label_mask]\n",
    "        acc = (y_pred == y_true).mean()\n",
    "        if best_acc < acc:\n",
    "            best_gain_type = gain_type\n",
    "            best_acc = acc\n",
    "    print(\"Best gain type is {} for class {}\".format(gain_type, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
