{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Import the required libraries.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re, random\n",
    "from copy import copy\n",
    "import sys\n",
    "from collections import Counter\n",
    "import queue\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('tagsets')\n",
    "# from nltk.corpus import stopwords\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import FreqDist\n",
    "from nltk import ngrams\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_text_labe(data):\n",
    "    text = []\n",
    "    answer_type = []\n",
    "    label = []\n",
    "    sent_length = []\n",
    "    for line in data:\n",
    "        a = line.split(':', maxsplit=1)\n",
    "        label.append(a[0])\n",
    "        b = a[1].strip().split(' ',maxsplit=1)\n",
    "        text.append(b[1].lower())\n",
    "        answer_type.append(b[0])\n",
    "    # remove punctuations\n",
    "    clean_text = [re.sub(r'([^\\w\\s]|[0-9])', ' ', line) for line in text]\n",
    "    clean_text = [re.sub(r'(\\s+)', ' ', line) for line in clean_text]\n",
    "    return clean_text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.add('')\n",
    "    text_tokens = [sent.split() for sent in text]\n",
    "    text_no_stopwords = [[w for w in words if w not in stop_words] for words in text_tokens]\n",
    "    return text_no_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def ngram_topk(token_list, n, k):\n",
    "    ngrams_list = [list(ngrams(sent, n)) for sent in token_list]\n",
    "    all_ngrams = sum(ngrams_list, [])\n",
    "    freq_dist = FreqDist(all_ngrams)\n",
    "    freq_dist_k = freq_dist.most_common(k)\n",
    "    ngrams_topk_list =  [ngram_token for ngram_token, _ in freq_dist_k]\n",
    "    ngrams_topk_dict = {ngram_token:i for i, ngram_token in enumerate(ngrams_topk_list)}\n",
    "    idx2ngram_dict = {v: k for k, v in ngrams_topk_dict.items()}\n",
    "    ngrams_freq_feat = []\n",
    "    for ngram_tokens in ngrams_list:\n",
    "        ngram_token_freq = np.zeros(k, dtype = np.int32)\n",
    "        for ngram_token in ngram_tokens:\n",
    "            if ngram_token in ngrams_topk_list:\n",
    "                ngram_token_freq[ ngrams_topk_dict[ngram_token] ]+=1\n",
    "        ngrams_freq_feat.append(ngram_token_freq)\n",
    "        \n",
    "    return np.asarray(ngrams_freq_feat, dtype = np.int32), ngrams_topk_dict, idx2ngram_dict, ngrams_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read the file line by line and clean the text of punctuation.\n",
    "with open('Data/train_5500.label', 'r',encoding='latin-1') as file:\n",
    "    data = file.readlines()\n",
    "\n",
    "X_train, Y_train = get_text_labe(data)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Todo:\n",
    "get vocab. (top 500)\n",
    "\n",
    "get sentence length feature (1)\n",
    "get lexical features: \n",
    "presence of n grams (1000)\n",
    "        1-gram (500)\n",
    "        2-gram (300)\n",
    "        3-gram (200)\n",
    "        \n",
    "Use nltk pos tagger and get tags\n",
    "for bag of tags model, assign presence to 1 iff the word is in 500 vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def getFeatures(X_train, feats_to_use = ['len', 'uni' ,'bi' ,'tri' ,'pos']):\n",
    "    X_train = [sent.split() for sent in X_train]\n",
    "    features = []\n",
    "    feat_dicts = []\n",
    "    if 'uni' in feats_to_use:\n",
    "        X_train_unigram = ngram_topk(X_train, 1, 500)\n",
    "        features.append(X_train_unigram[0])\n",
    "        feat_dicts.append(X_train_unigram[1:])\n",
    "    if 'bi' in feats_to_use:\n",
    "        X_train_bigram = ngram_topk(X_train, 2, 300)\n",
    "        features.append(X_train_bigram[0])\n",
    "        feat_dicts.append(X_train_bigram[1:])\n",
    "    if 'tri' in feats_to_use:\n",
    "        X_train_trigram = ngram_topk(X_train, 3, 200)\n",
    "        features.append(X_train_trigram[0])\n",
    "        feat_dicts.append(X_train_trigram[1:])\n",
    "    if 'pos' in feats_to_use:\n",
    "        X_train_pos = [pos_tag(tokens) for tokens in X_train]\n",
    "        X_train_pos_unigram = ngram_topk(X_train_pos, 1, 500)\n",
    "        features.append(X_train_pos[0])\n",
    "        feat_dicts.append(X_train_pos[1:])\n",
    "    if 'len' in feats_to_use:\n",
    "        X_train_sentlen = np.reshape(np.asarray([len(sent) for sent in X_train], dtype = np.int32),(-1,1))\n",
    "        features.append(X_train_sentlen)\n",
    "        feat_dicts.append(None)\n",
    "        \n",
    "    X_train_feats = np.concatenate( features, axis=1 )\n",
    "    return X_train_feats, feat_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sublists(input_list):\n",
    "    subs = []\n",
    "\n",
    "    for i in range(0, len(input_list) + 1):\n",
    "        temp = [list(x) for x in combinations(input_list, i)]\n",
    "\n",
    "        if len(temp) > 0:\n",
    "            subs.extend(temp)\n",
    "\n",
    "    return subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Needs Work\n",
    "def tester(X, Y, iterations = 1):\n",
    "    kf = KFold(n_splits = 10, shuffle = True)\n",
    "    scores = []\n",
    "    mean_scores = []\n",
    "\n",
    "    for i in range(iterations):\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            train_length = len(train_index)\n",
    "            valid_index = train_index[: train_length // 10]\n",
    "            train_index = train_index[train_length // 10 :]\n",
    "            X_train, X_test = X.iloc[train_index].drop(['index'], axis = 1),\n",
    "                              X.iloc[test_index].drop(['index'], axis = 1)\n",
    "            Y_train, Y_test = Y.iloc[train_index].drop(['index'], axis = 1),\n",
    "                              Y.iloc[test_index].drop(['index'], axis = 1)\n",
    "            clf = linear_model.LogisticRegression(solver = 'liblinear', penalty = 'l2',\n",
    "                  max_iter = 200).fit(X_train, Y_train.values.ravel())\n",
    "            scores.append(clf.score(X_test, Y_test))\n",
    "\n",
    "        mean_scores.append(np.mean(scores))\n",
    "\n",
    "    return np.mean(mean_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Needs Work\n",
    "\n",
    "power_metrics = sublists(metrics)\n",
    "max_score = -1\n",
    "\n",
    "for metric_list in power_metrics:\n",
    "    if metric_list == []:\n",
    "        continue\n",
    "\n",
    "    X = data[metric_list].reset_index()\n",
    "    Y = data['Diabetic'].reset_index()\n",
    "    score = tester(X, Y, 1, False)\n",
    "\n",
    "    if score > max_score:\n",
    "        max_score = score\n",
    "        print(metric_list)\n",
    "        print(max_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Node():\n",
    "    cnt = 0\n",
    "    def __init__(self, ):\n",
    "        self.leaf = False\n",
    "        self.majority_class = None\n",
    "        self.attribute_index = None\n",
    "        self.children = dict() # key: attribute_value, value: child_node\n",
    "        self.id = Node.cnt\n",
    "        Node.cnt+=1\n",
    "    \n",
    "    def __str__(self,):\n",
    "        print('ID: {}  isLeaf: {} majority: {} split_idx: {} split_val = {}'.format(self.id, \n",
    "                                                                                    self.leaf, \n",
    "                                                                                    self.majority_class, \n",
    "                                                                                    self.attribute_index, \n",
    "                                                                                    list(self.children.keys())\n",
    "                                                                                   ))          \n",
    "    def traverse_print(self,):\n",
    "        print(self)\n",
    "        for _, child in self.children:\n",
    "              child.traverse_print()\n",
    "              \n",
    "              \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "    \n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_entropy(labels):\n",
    "        entropy = 0.0\n",
    "        totSamples = len(labels)\n",
    "        labelSet = set(labels.reshape(-1))\n",
    "        for label in labelSet:\n",
    "            prob = np.sum(labels == label) / totSamples\n",
    "            entropy -= np.log(prob) * prob\n",
    "        return entropy\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_dict_entropy(attr_count):\n",
    "        entropy = 0\n",
    "        totSamples = sum(attr_count.values())\n",
    "        \n",
    "        labelSet = attr_count.keys()\n",
    "        for label in labelSet:\n",
    "            prob = attr_count[label] / totSamples\n",
    "            entropy -= np.log(prob) * prob\n",
    "        return entropy\n",
    "    \n",
    "    def split_node(self, data, labels, parent, used_attr_index):\n",
    "        num_instances = data.shape[0]\n",
    "        parent_info = compute_entropy(labels) * num_instances\n",
    "        \n",
    "        parent.majority_class = Counter(labels.reshape(-1)).most_common(1)[0][0]\n",
    "                \n",
    "        if parent_info == 0 :\n",
    "            parent.leaf = True\n",
    "        \n",
    "        best_attr_index = None\n",
    "        best_info_gain = -float('inf')\n",
    "        best_gain_ratio = -float('inf')\n",
    "        best_attr_keys = None\n",
    "        \n",
    "        # sent length case special\n",
    "        attr_split_info = 0\n",
    "        attr_count = dict()\n",
    "        sent_len_split_val = stats.mode(a[:, 0])[0][0]\n",
    "        le_ids = np.where(data[:, 0] <= sent_len_split_val)[0]\n",
    "        gt_ids = np.where(data[:, 0] ? sent_len_split_val)[0]\n",
    "        attr_count[0] = le_ids.shape[0]\n",
    "        attr_count[1] = gt_ids.shape[0]\n",
    "        attr_split_info = (attr_count[0] * compute_entropy(labels[le_ids])) + (attr_count[1] * compute_entropy(labels[gt_ids]) )    \n",
    "        attr_gain = parent_info - attr_split_info\n",
    "        attr_gain_ratio = compute_dict_entropy(attr_count) * attr_gain\n",
    "        if best_gain_ratio < attr_gain_ratio and  attr_gain_ratio > 0 :\n",
    "                best_attr_index = 0\n",
    "                best_info_gain = attr_gain\n",
    "                best_gain_ratio = attr_gain_ratio\n",
    "                best_attr_keys = attr_count.keys()\n",
    "            \n",
    "        # during ablation, sentence length can be initialized to all zeros this will prevent splittiung in sent dimension/.\n",
    "        for i in range(1, len(data.shape[1])): # starts from 1 as zero is sentence length (always.) . \n",
    "            if i in used_indices:\n",
    "                continue\n",
    "            attr_split_info = 0\n",
    "            attr_count = dict()\n",
    "            for attr_val in set(data[:, i].reshape(-1)):\n",
    "                ids = np.where(data[:, i] == attr_val)[0]\n",
    "                attr_count[attr_val] = len(ids)\n",
    "                attr_split_info += attr_count[attr_val] * compute_entropy(labels[ids])\n",
    "            attr_gain = parent_info - attr_split_info\n",
    "            attr_gain_ratio = compute_dict_entropy(attr_count) * attr_gain\n",
    "            \n",
    "            if best_gain_ratio < attr_gain_ratio:\n",
    "                best_attr_index = i\n",
    "                best_info_gain = attr_gain\n",
    "                best_gain_ratio = attr_gain_ratio\n",
    "                best_attr_keys = attr_count.keys()\n",
    "        if best_gain_ratio <= 0 :\n",
    "            parent.leaf = True\n",
    "            return [] # TO Check    \n",
    "        else:\n",
    "            parent.attribute_index =  best_attr_index\n",
    "            parent.children = { i: Node() for i in best_attr_keys}\n",
    "            to_return = []\n",
    "            used_attr_index.append(best_attr_index)\n",
    "            if best_attr_index != 0:\n",
    "                for i in best_attr_keys:\n",
    "                    inds = np.where(data[:, best_attr_index] == i)[0]\n",
    "                    to_return.append( (parent.children[i], data[inds], labels[inds], used_attr_index) )\n",
    "            else:\n",
    "                to_return.append( (parent.children[0], data[le_ids], labels[le_ids], used_attr_index) )\n",
    "                to_return.append( (parent.children[1], data[gt_ids], labels[gt_ids], used_attr_index) )\n",
    "            return to_return\n",
    "        \n",
    "    def build_tree(self, data, labels):\n",
    "        traversal_q = queue.Queue()\n",
    "        root = Node()\n",
    "        traversal_q.put_nowait( (root, data, labels, [] ))\n",
    "        while not traversal_q.empty():\n",
    "            node_to_split = traversal_q.get_nowait()\n",
    "            child_nodes = split_node(*node_to_split)\n",
    "            for child in child_nodes:\n",
    "                traversal_q.put_nowait(child)\n",
    "        \n",
    "        return root\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
