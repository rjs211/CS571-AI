{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the required libraries.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re, random\n",
    "from copy import copy\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('tagsets')\n",
    "# from nltk.corpus import stopwords\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import FreqDist\n",
    "from nltk import ngrams\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_text_labe(data):\n",
    "    text = []\n",
    "    answer_type = []\n",
    "    label = []\n",
    "    sent_length = []\n",
    "    for line in data:\n",
    "        a = line.split(':', maxsplit=1)\n",
    "        label.append(a[0])\n",
    "        b = a[1].strip().split(' ',maxsplit=1)\n",
    "        text.append(b[1].lower())\n",
    "        answer_type.append(b[0])\n",
    "    # remove punctuations\n",
    "    clean_text = [re.sub(r'([^\\w\\s]|[0-9])', ' ', line) for line in text]\n",
    "    clean_text = [re.sub(r'(\\s+)', ' ', line) for line in clean_text]\n",
    "    return clean_text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.add('')\n",
    "    text_tokens = [sent.split() for sent in text]\n",
    "    text_no_stopwords = [[w for w in words if w not in stop_words] for words in text_tokens]\n",
    "    return text_no_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_topk(token_list, n, k):\n",
    "    ngrams_list = [list(ngrams(sent, n)) for sent in token_list]\n",
    "    all_ngrams = sum(ngrams_list, [])\n",
    "    freq_dist = FreqDist(all_ngrams)\n",
    "    freq_dist_k = freq_dist.most_common(k)\n",
    "    ngrams_topk_list =  [ngram_token for ngram_token, _ in freq_dist_k]\n",
    "    ngrams_topk_dict = {ngram_token:i for i, ngram_token in enumerate(ngrams_topk_list)}\n",
    "    idx2ngram_dict = {v: k for k, v in ngrams_topk_dict.items()}\n",
    "    ngrams_freq_feat = []\n",
    "    for ngram_tokens in ngrams_list:\n",
    "        ngram_token_freq = np.zeros(k, dtype = np.int32)\n",
    "        for ngram_token in ngram_tokens:\n",
    "            if ngram_token in ngrams_topk_list:\n",
    "                ngram_token_freq[ ngrams_topk_dict[ngram_token] ]+=1\n",
    "        ngrams_freq_feat.append(ngram_token_freq)\n",
    "        \n",
    "    return np.asarray(ngrams_freq_feat, dtype = np.int32), ngrams_topk_dict, idx2ngram_dict, ngrams_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file line by line and clean the text of punctuation.\n",
    "with open('Data/train_5500.label', 'r',encoding='latin-1') as file:\n",
    "    data = file.readlines()\n",
    "\n",
    "X_train, Y_train = get_text_labe(data)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo:\n",
    "get vocab. (top 500)\n",
    "\n",
    "get sentence length feature (1)\n",
    "get lexical features: \n",
    "presence of n grams (1000)\n",
    "        1-gram (500)\n",
    "        2-gram (300)\n",
    "        3-gram (200)\n",
    "        \n",
    "Use nltk pos tagger and get tags\n",
    "for bag of tags model, assign presence to 1 iff the word is in 500 vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeatures(X_train, feats_to_use = ['len', 'uni' ,'bi' ,'tri' ,'pos']):\n",
    "    X_train = [sent.split() for sent in X_train]\n",
    "    features = []\n",
    "    feat_dicts = []\n",
    "    if 'uni' in feats_to_use:\n",
    "        X_train_unigram = ngram_topk(X_train, 1, 500)\n",
    "        features.append(X_train_unigram[0])\n",
    "        feat_dicts.append(X_train_unigram[1:])\n",
    "    if 'bi' in feats_to_use:\n",
    "        X_train_bigram = ngram_topk(X_train, 2, 300)\n",
    "        features.append(X_train_bigram[0])\n",
    "        feat_dicts.append(X_train_bigram[1:])\n",
    "    if 'tri' in feats_to_use:\n",
    "        X_train_trigram = ngram_topk(X_train, 3, 200)\n",
    "        features.append(X_train_trigram[0])\n",
    "        feat_dicts.append(X_train_trigram[1:])\n",
    "    if 'pos' in feats_to_use:\n",
    "        X_train_pos = [pos_tag(tokens) for tokens in X_train]\n",
    "        X_train_pos_unigram = ngram_topk(X_train_pos, 1, 500)\n",
    "        features.append(X_train_pos[0])\n",
    "        feat_dicts.append(X_train_pos[1:])\n",
    "    if 'len' in feats_to_use:\n",
    "        X_train_sentlen = np.reshape(np.asarray([len(sent) for sent in X_train], dtype = np.int32),(-1,1))\n",
    "        features.append(X_train_sentlen)\n",
    "        feat_dicts.append(None)\n",
    "        \n",
    "    X_train_feats = np.concatenate( features, axis=1 )\n",
    "    return X_train_feats, feat_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sublists(input_list):\n",
    "    subs = []\n",
    "\n",
    "    for i in range(0, len(input_list) + 1):\n",
    "        temp = [list(x) for x in combinations(input_list, i)]\n",
    "\n",
    "        if len(temp) > 0:\n",
    "            subs.extend(temp)\n",
    "\n",
    "    return subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needs Work\n",
    "def tester(X, Y, iterations = 1):\n",
    "    kf = KFold(n_splits = 10, shuffle = True)\n",
    "    scores = []\n",
    "    mean_scores = []\n",
    "\n",
    "    for i in range(iterations):\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            train_length = len(train_index)\n",
    "            valid_index = train_index[: train_length // 10]\n",
    "            train_index = train_index[train_length // 10 :]\n",
    "            X_train, X_test = X.iloc[train_index].drop(['index'], axis = 1),\n",
    "                              X.iloc[test_index].drop(['index'], axis = 1)\n",
    "            Y_train, Y_test = Y.iloc[train_index].drop(['index'], axis = 1),\n",
    "                              Y.iloc[test_index].drop(['index'], axis = 1)\n",
    "            clf = linear_model.LogisticRegression(solver = 'liblinear', penalty = 'l2',\n",
    "                  max_iter = 200).fit(X_train, Y_train.values.ravel())\n",
    "            scores.append(clf.score(X_test, Y_test))\n",
    "\n",
    "        mean_scores.append(np.mean(scores))\n",
    "\n",
    "    return np.mean(mean_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Needs Work\n",
    "\n",
    "power_metrics = sublists(metrics)\n",
    "max_score = -1\n",
    "\n",
    "for metric_list in power_metrics:\n",
    "    if metric_list == []:\n",
    "        continue\n",
    "\n",
    "    X = data[metric_list].reset_index()\n",
    "    Y = data['Diabetic'].reset_index()\n",
    "    score = tester(X, Y, 1, False)\n",
    "\n",
    "    if score > max_score:\n",
    "        max_score = score\n",
    "        print(metric_list)\n",
    "        print(max_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
