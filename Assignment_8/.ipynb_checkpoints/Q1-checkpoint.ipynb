{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Import the required libraries.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re, random\n",
    "from copy import copy\n",
    "import sys\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('tagsets')\n",
    "# from nltk.corpus import stopwords\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import FreqDist\n",
    "from nltk import ngrams\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_text_labe(data):\n",
    "    text = []\n",
    "    answer_type = []\n",
    "    label = []\n",
    "    sent_length = []\n",
    "    for line in data:\n",
    "        a = line.split(':', maxsplit=1)\n",
    "        label.append(a[0])\n",
    "        b = a[1].strip().split(' ',maxsplit=1)\n",
    "        text.append(b[1].lower())\n",
    "        answer_type.append(b[0])\n",
    "    # remove punctuations\n",
    "    clean_text = [re.sub(r'([^\\w\\s]|[0-9])', ' ', line) for line in text]\n",
    "    clean_text = [re.sub(r'(\\s+)', ' ', line) for line in clean_text]\n",
    "    return clean_text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.add('')\n",
    "    text_tokens = [sent.split() for sent in text]\n",
    "    text_no_stopwords = [[w for w in words if w not in stop_words] for words in text_tokens]\n",
    "    return text_no_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def ngram_topk(token_list, n, k):\n",
    "    ngrams_list = [list(ngrams(sent, n)) for sent in token_list]\n",
    "    all_ngrams = sum(ngrams_list, [])\n",
    "    freq_dist = FreqDist(all_ngrams)\n",
    "    freq_dist_k = freq_dist.most_common(k)\n",
    "    ngrams_topk_list =  [ngram_token for ngram_token, _ in freq_dist_k]\n",
    "    ngrams_topk_dict = {ngram_token:i for i, ngram_token in enumerate(ngrams_topk_list)}\n",
    "    idx2ngram_dict = {v: k for k, v in ngrams_topk_dict.items()}\n",
    "    ngrams_freq_feat = []\n",
    "    for ngram_tokens in ngrams_list:\n",
    "        ngram_token_freq = np.zeros(k, dtype = np.int32)\n",
    "        for ngram_token in ngram_tokens:\n",
    "            if ngram_token in ngrams_topk_list:\n",
    "                ngram_token_freq[ ngrams_topk_dict[ngram_token] ]+=1\n",
    "        ngrams_freq_feat.append(ngram_token_freq)\n",
    "        \n",
    "    return np.asarray(ngrams_freq_feat, dtype = np.int32), ngrams_topk_dict, idx2ngram_dict, ngrams_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read the file line by line and clean the text of punctuation.\n",
    "with open('Data/train_5500.label', 'r',encoding='latin-1') as file:\n",
    "    data = file.readlines()\n",
    "\n",
    "X_train, Y_train = get_text_labe(data)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Todo:\n",
    "get vocab. (top 500)\n",
    "\n",
    "get sentence length feature (1)\n",
    "get lexical features: \n",
    "presence of n grams (1000)\n",
    "        1-gram (500)\n",
    "        2-gram (300)\n",
    "        3-gram (200)\n",
    "        \n",
    "Use nltk pos tagger and get tags\n",
    "for bag of tags model, assign presence to 1 iff the word is in 500 vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def getFeatures(X_train, feats_to_use = ['len', 'uni' ,'bi' ,'tri' ,'pos']):\n",
    "    X_train = [sent.split() for sent in X_train]\n",
    "    features = []\n",
    "    feat_dicts = []\n",
    "    if 'uni' in feats_to_use:\n",
    "        X_train_unigram = ngram_topk(X_train, 1, 500)\n",
    "        features.append(X_train_unigram[0])\n",
    "        feat_dicts.append(X_train_unigram[1:])\n",
    "    if 'bi' in feats_to_use:\n",
    "        X_train_bigram = ngram_topk(X_train, 2, 300)\n",
    "        features.append(X_train_bigram[0])\n",
    "        feat_dicts.append(X_train_bigram[1:])\n",
    "    if 'tri' in feats_to_use:\n",
    "        X_train_trigram = ngram_topk(X_train, 3, 200)\n",
    "        features.append(X_train_trigram[0])\n",
    "        feat_dicts.append(X_train_trigram[1:])\n",
    "    if 'pos' in feats_to_use:\n",
    "        X_train_pos = [pos_tag(tokens) for tokens in X_train]\n",
    "        X_train_pos_unigram = ngram_topk(X_train_pos, 1, 500)\n",
    "        features.append(X_train_pos[0])\n",
    "        feat_dicts.append(X_train_pos[1:])\n",
    "    if 'len' in feats_to_use:\n",
    "        X_train_sentlen = np.reshape(np.asarray([len(sent) for sent in X_train], dtype = np.int32),(-1,1))\n",
    "        features.append(X_train_sentlen)\n",
    "        feat_dicts.append(None)\n",
    "        \n",
    "    X_train_feats = np.concatenate( features, axis=1 )\n",
    "    return X_train_feats, feat_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sublists(input_list):\n",
    "    subs = []\n",
    "\n",
    "    for i in range(0, len(input_list) + 1):\n",
    "        temp = [list(x) for x in combinations(input_list, i)]\n",
    "\n",
    "        if len(temp) > 0:\n",
    "            subs.extend(temp)\n",
    "\n",
    "    return subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Needs Work\n",
    "def tester(X, Y, iterations = 1):\n",
    "    kf = KFold(n_splits = 10, shuffle = True)\n",
    "    scores = []\n",
    "    mean_scores = []\n",
    "\n",
    "    for i in range(iterations):\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            train_length = len(train_index)\n",
    "            valid_index = train_index[: train_length // 10]\n",
    "            train_index = train_index[train_length // 10 :]\n",
    "            X_train, X_test = X.iloc[train_index].drop(['index'], axis = 1),\n",
    "                              X.iloc[test_index].drop(['index'], axis = 1)\n",
    "            Y_train, Y_test = Y.iloc[train_index].drop(['index'], axis = 1),\n",
    "                              Y.iloc[test_index].drop(['index'], axis = 1)\n",
    "            clf = linear_model.LogisticRegression(solver = 'liblinear', penalty = 'l2',\n",
    "                  max_iter = 200).fit(X_train, Y_train.values.ravel())\n",
    "            scores.append(clf.score(X_test, Y_test))\n",
    "\n",
    "        mean_scores.append(np.mean(scores))\n",
    "\n",
    "    return np.mean(mean_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Needs Work\n",
    "\n",
    "power_metrics = sublists(metrics)\n",
    "max_score = -1\n",
    "\n",
    "for metric_list in power_metrics:\n",
    "    if metric_list == []:\n",
    "        continue\n",
    "\n",
    "    X = data[metric_list].reset_index()\n",
    "    Y = data['Diabetic'].reset_index()\n",
    "    score = tester(X, Y, 1, False)\n",
    "\n",
    "    if score > max_score:\n",
    "        max_score = score\n",
    "        print(metric_list)\n",
    "        print(max_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Node():\n",
    "    \n",
    "    def __init__(self, ):\n",
    "        self.leaf = False\n",
    "        self.majority_class = None\n",
    "        self.attribute_index = None\n",
    "        self.children = dict() # key: attribute_value, value: child_node\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "    \n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_entropy(labels):\n",
    "        entropy = 0.0\n",
    "        totSamples = len(labels)\n",
    "        labelSet = set(labels.reshape(-1))\n",
    "        for label in labelSet:\n",
    "            prob = np.sum(labels == label) / totSamples\n",
    "            entropy -= np.log(prob) * prob\n",
    "        return entropy\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_dict_entropy(attr_count):\n",
    "        entropy = 0\n",
    "        totSamples = sum(attr_count.values())\n",
    "        \n",
    "        labelSet = attr_count.keys()\n",
    "        for label in labelSet:\n",
    "            prob = attr_count[label] / totSamples\n",
    "            entropy -= np.log(prob) * prob\n",
    "        return entropy\n",
    "    \n",
    "    def buildTree(self, data, labels, parent, used_attr_index):\n",
    "        num_instances = data.shape[0]\n",
    "        parent_info = compute_entropy(labels) * num_instances\n",
    "        \n",
    "        parent.majority_class = Counter(labels.reshape(-1)).most_common(1)[0][0]\n",
    "                \n",
    "        if parent_info == 0 :\n",
    "            parent.leaf = True\n",
    "        \n",
    "        best_attr_index = None\n",
    "        best_info_gain = -float('inf')\n",
    "        best_gain_ratio = -float('inf')\n",
    "        best_attr_keys = None\n",
    "        for i in range(len(data.shape[1])):\n",
    "            if i in used_indices:\n",
    "                continue\n",
    "            attr_split_info = 0\n",
    "            attr_count = dict()\n",
    "            for attr_val in set(data[:, i].reshape(-1)):\n",
    "                ids = np.where(data[:, i] == attr_val)[0]\n",
    "                attr_count[attr_val] = len(ids)\n",
    "                attr_split_info += attr_count[attr_val] * compute_entropy(labels[ids])\n",
    "            attr_gain = parent_info - attr_split_info\n",
    "            attr_gain_ratio = compute_dict_entropy(attr_count) * attr_gain\n",
    "            \n",
    "            if best_gain_ratio < attr_gain_ratio:\n",
    "                best_attr_index = i\n",
    "                best_info_gain = attr_gain\n",
    "                best_gain_ratio = attr_gain_ratio\n",
    "                best_attr_keys = attr_count.keys()\n",
    "        if best_gain_ratio <= 0 :\n",
    "            parent.leaf = True\n",
    "            return None # TO Check    \n",
    "        else:\n",
    "            parent.attribute_index =  best_attr_index\n",
    "            parent.children = { i: Node() for i in best_attr_keys}\n",
    "            toret = []\n",
    "            used_attr_index.append(best_attr_index)\n",
    "            for i in best_attr_keys:\n",
    "                inds = np.where(data[:, best_attr_index] == i)[0]\n",
    "                toret.append( (parent.children[i], data[inds], labels[inds], used_attr_index) )\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "195"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.where(np.arange(25)> 14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2,  3,  4,  5],\n",
       "       [ 6,  7,  8,  9, 10, 11],\n",
       "       [12, 13, 14, 15, 16, 17],\n",
       "       [18, 19, 20, 21, 22, 23],\n",
       "       [24, 25, 26, 27, 28, 29]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = np.arange(30).reshape(5,6)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  9, 15, 21, 27])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
