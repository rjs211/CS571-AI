# -*- coding: utf-8 -*-
"""Working Copy of Muks' copy of MLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HvsXQEryZ-bLhXD0nLLhCmyS5uUepqWv

# **Layer Implementation**
"""

import numpy as np
import pandas as pd
import random
from copy import deepcopy

from sklearn.model_selection import StratifiedShuffleSplit

import matplotlib
import matplotlib.pyplot as plt

random.seed(42)
np.random.seed(42)

deb = print

class FullyConnected():
    layer_count = 0
    def __init__(self, out_features = 100, in_features = 100):
        '''
            Fully Connected layer
            out_features: No of Output Features
            no_of_inp_featues: No of Input Features
        '''
        self.out_features = out_features
        self.in_features = in_features
        self.weight = np.random.normal(loc=0.0, scale=1/np.sqrt(self.out_features), size=(self.out_features, self.in_features))
        self.bias = np.zeros((self.out_features, 1), dtype=np.float32)
        self.weight_grad = None
        self.bias_grad = None
        self.layer_id = FullyConnected.layer_count
        FullyConnected.layer_count += 1
    
    def __str__(self, ):
        return 'Fully Connected Layer ID = {}: (In:{}, Out:{})'.format(self.layer_id, self.in_features, self.out_features)
    
    def forward(self, x):
        '''
            x: Input (N, F_in) where N = No. of samples, F_in: No. of input features
            
            returns:
                Wx + B (N, F_out) where W, B are trainable weight and bias where N: No. of samples, F_out Output dimension/features.
        '''
        assert x.shape[1] == self.in_features, "Input dimension Mismatch"
        wx = np.matmul(self.weight, np.transpose(x, (1, 0)))
        wx_plus_b = np.add(wx, self.bias)
        return np.transpose(wx_plus_b, (1, 0))
    
    def backward(self, y_grad, y, x):
        '''
            y_grad: Gradiant of output (N, F_out)
            y: Output of layer (N, F_out)
            x: input of layer (N, F_in)
        '''
        grad_x = np.matmul(y_grad, self.weight)
        
        weight_grad = np.matmul(np.transpose(y_grad, (1, 0)), x)
        if self.weight_grad is not None:
          self.weight_grad += weight_grad
        else:
          self.weight_grad = weight_grad
        assert y_grad.shape == y.shape, "y_grad.shape: {}, y.shape: {}".format(y_grad.shape, y.shape)

        bias_grad = np.sum(np.transpose(y_grad, (1, 0)), axis=1, keepdims=True)
        assert bias_grad.shape == self.bias.shape, "bias_grad.shape: {}, self.bias.shape: {}".format(bias_grad.shape, self.bias.shape)
        if self.bias_grad is not None:
          self.bias_grad += bias_grad
        else:
          self.bias_grad = bias_grad
        return grad_x
    
    def apply_gradients(self, lr=0.001):
        
        self.weight -= lr * self.weight_grad
        self.bias -= lr * self.bias_grad

        convergence_criteria = np.sum(np.absolute(self.weight_grad)) + np.sum(np.absolute(self.bias_grad))
        self.weight_grad = None
        self.bias_grad = None
        deb(convergence_criteria)

        if convergence_criteria < 1e-3:
            return True
        else:
            return False
    
    def __call__(self, *args, **kwargs):
        return self.forward(*args, **kwargs)

"""### **Testing FullyConnected**"""

'''
x = np.random.randn(4, 100)
fl1 = FullyConnected()
y = fl1.forward(x)
g = fl1.backward(y,y,x)
fl1.apply_gradients()
'''

#g.shape

class SoftmaxCrossEntropyLoss():
  """
    Combines a softmax layer with the cross-entropy loss
  """
  layer_count = 0
  def __init__(self):
      self.layer_id = SoftmaxCrossEntropyLoss.layer_count
      SoftmaxCrossEntropyLoss.layer_count += 1

  def __str__(self, ):
      return 'Softmax Cross Entropy Loss Layer ID = {}'.format(self.layer_id)

  def forward(self, x, y_true):
    '''
      x: Input (N, C) where N = No. of samples, C = No. of classes
      y_true: Target (N, C), boolean values
    '''
    # Softmax
    x_stable = x - np.max(x, axis = 1, keepdims = True)
    p = np.exp(x_stable) / np.sum(np.exp(x_stable), axis = 1, keepdims = True)
    
    # Cross Entropy Loss
    loss = y_true * (- x_stable + np.log(np.sum(np.exp(x_stable), axis = 1, keepdims = True)))
    loss = np.mean(np.sum(loss, axis = -1))
    return p, loss
  
  def backward(self, y_pred, y_true):
    '''
      y_pred: (N, C)
      y_true: (N, C)
    '''
    # Using y_pred.shape[0] as a workaround for batch_size.
    return (y_pred - y_true) / y_pred.shape[0]
  
  def __call__(self, *args, **kwargs):
    return self.forward(*args, **kwargs)
  
  def apply_gradients(self, lr=None):
    return True

class Sigmoid():
  """
  Class for implementing sigmoid layer.
  """
  layer_count = 0
  def __init__(self):
    self.layer_id = Sigmoid.layer_count
    Sigmoid.layer_count += 1
    
  def __str__(self, ):
      return 'Sigmoid Layer ID = {}'.format(self.layer_id)

  def forward(self, x):
    """
    Function for implemeting the sigmoid expression.

    x: Input(N, C) where N = number of samples, C = number of classes.
    """
    y = 1.0 / (1.0 + np.exp(-x))
    return y

  def backward(self, y_grad, y, x):
    """
    Function for calculating the gradients of the current sigmoid perceptron.

    y_grad: Gradient at the output.
    y: The calculated forward(x).
    """
    w_grad = y_grad * y * (1 - y)
    return w_grad

  def __call__(self, *args, **kwargs):
    return self.forward(*args, **kwargs)
  
  def apply_gradients(self, lr=None):
    return True

class Tanh():
  """
  Class for implementing tanh layer.
  """
  layer_count = 0
  def __init__(self):
    self.layer_id = Tanh.layer_count
    Tanh.layer_count += 1

  def __str__(self, ):
      return 'Tanh Layer ID = {}'.format(self.layer_id)

  def forward(self, x):
    """
    Function for implemeting the tanh expression.

    x: Input(N, C) where N = number of samples, C = number of classes.
    """
    y = np.tanh(x)
    return y

  def backward(self, y_grad, y, x):
    """
    Function for calculating the gradients of the current sigmoid perceptron.

    y_grad: Gradient at the output.
    y: The calculated forward(x).
    """
    w_grad = y_grad * (1 - (y ** 2))
    return w_grad

  def __call__(self, *args, **kwargs):
    return self.forward(*args, **kwargs)

  def apply_gradients(self, lr=None):
    return True

class SoftmaxLayer():
  """
  Class implementing a softmax layer
  """
  layer_count = 0
  def __init__(self):
    self.layer_id = SoftmaxLayer.layer_count
    SoftmaxLayer.layer_count += 1
  
  def __str__(self, ):
      return 'Softmax Layer ID = {}'.format(self.layer_id)

  def forward(self, x):
    '''
      x: Input (N, C) where N = No. of samples, C = No. of classes
    '''
    # Softmax
    x_stable = x - np.max(x, axis = 1, keepdims = True)
    p = np.exp(x_stable) / np.sum(np.exp(x_stable), axis = 1, keepdims = True)
    
    return p
  
  def backward(self, y_grad, y, x):
    '''
    y_grad: Gradient at output
    y: (N, C)
    '''
    raise NotImplementedError("Backprop not implemented for SoftmaxLayer, please use SoftmaxCrossEntropyLoss instead.")
  
  def __call__(self, *args, **kwargs):
    return self.forward(*args, **kwargs)
  
  def apply_gradients(self, lr=None):
    return True

class SigmoidCrossEntropyLoss():
  """
    Combines a sigmoid layer with the cross-entropy loss.
  """
  layer_count = 0
  def __init__(self):
    self.layer_id = SigmoidCrossEntropyLoss.layer_count
    SigmoidCrossEntropyLoss.layer_count += 1

  def __str__(self, ):
    return 'Softmax Cross Entropy Loss Layer ID = {}'.format(self.layer_id)

  def forward(self, x, y_true):
    '''
      x: Input (N, C) where N = No. of samples, C = No. of classes
      y_true: Target (N, C), boolean values
    '''
    # Sigmoid
    p = 1.0 / (1.0 + np.exp(-x))
    
    # Cross Entropy Loss
    x_stable = np.maximum(x, 0)
    loss = x_stable - x * y_true + np.log(1 + np.exp(np.absolute(x)))
    loss = np.mean(np.sum(loss, axis = -1))
    return p, loss
  
  def backward(self, y_pred, y_true):
    '''
      y_pred: (N, C)
      y_true: (N, C)
    '''
    # Using y_pred.shape[0] as a workaround for batch_size.
    return (y_pred - y_true) / y_pred.shape[0]
  
  def __call__(self, *args, **kwargs):
    return self.forward(*args, **kwargs)
  
  def apply_gradients(self, lr=None):
    return True

class SigmoidMSELoss():
  """
    Combines a sigmoid layer with the mean squared error loss.
  """
  layer_count = 0
  def __init__(self):
    self.layer_id = SigmoidMSELoss.layer_count
    SigmoidMSELoss.layer_count += 1

  def __str__(self, ):
      return 'Sigmoid Mean Squared Error Loss Layer ID = {}'.format(self.layer_id)

  def forward(self, x, y_true):
    '''
      x: Input (N, C) where N = No. of samples, C = No. of classes
      y_true: Target (N, C), boolean values
    '''
    # Sigmoid
    p = 1.0 / (1.0 + np.exp(-x))
    
    # Mean Squared Error Loss
    squared_error = (y_true - p) ** 2
    assert len(squared_error.shape) == 2
    loss = (np.sum(squared_error, axis = -1)) / 2
    loss = np.mean(loss, axis=-1)
    return p, loss
  
  def backward(self, y_pred, y_true):
    '''
      y_pred: (N, C)
      y_true: (N, C)
    '''
    # Using y_pred.shape[0] as a workaround for batch_size.
    return -(y_true - y_pred) * y_pred * (1 - y_pred) / y_pred.shape[0]
  
  def __call__(self, *args, **kwargs):
    return self.forward(*args, **kwargs)
  
  def apply_gradients(self, lr=None):
    return True

class MultiLayerPerceptron():
  def __init__(self, layers, loss_layer, final_layer):
    self.layers = layers
    self.loss_layer = loss_layer
    self.final_layer = final_layer
    self.all_outputs = []
    self.train = True

  def __str__(self, ):
      summary = []
      for layer in self.layers:
          summary.append(str(layer))
      if self.final_layer is not None:
          summary.append('Final Layer: ' + str(self.final_layer))
      if self.loss_layer is not None:
        summary.append('Loss Layer: ' + str(self.loss_layer))
      return ' \n'.join(summary)
  
  def forward(self, x, y_true=None):
    # Training mode
    if self.train:
      assert (self.loss_layer is not None) and (y_true is not None), "Training mode, please pass y_true and set a loss layer"
      all_outputs = [x]
      
      for layer in self.layers:
        print(layer, x.shape, end=', ')
        x = layer(x)
        all_outputs.append(x)
        print(x.shape)
        # deb(layer)
      
      pred, loss = self.loss_layer(x, y_true)
      all_outputs.append((pred, y_true))
      
      # deb(len(all_outputs))
      # Save outputs for multiple forward passes
      self.all_outputs.append(all_outputs)
    
    # Evaluation mode
    else:
      assert (self.final_layer is not None) and (y_true is None), "Evaluation mode, doesn't take y_true as input"
      all_outputs = [x]
      
      for layer in self.layers:
        x = layer(x)
        all_outputs.append(x)
      
      pred = self.final_layer(x)
      loss = None
      all_outputs.append((pred, y_true))
    
    return all_outputs, loss   
  
  def backward(self):
      assert self.train

      for all_outputs in self.all_outputs[::-1]:
        y_grad = self.loss_layer.backward(*all_outputs[-1])
        all_outputs = all_outputs[:-1]
        for idx, layer in enumerate(self.layers[::-1]):
          # Pass the grad at the output, output of the layer, the input of the layer to backward()
          y_grad = layer.backward(y_grad, all_outputs[-idx-1], all_outputs[-idx-2])
      
      self.all_outputs = []
      return

  def train_mode(self):
      '''
      Set train mode
      '''
      self.train = True
    
  def eval_mode(self):
      '''
      Set eval mode
      '''
      self.train = False
    
  def optimize(self, learning_rate):
      '''
      Optimizes
      '''
      conv = True
      for layer in self.layers:
        conv = layer.apply_gradients(learning_rate) and conv
      return conv

layers = [
            FullyConnected(2, 2),
            Sigmoid(),
            FullyConnected(1, 2),
          ]
mymlp = MultiLayerPerceptron(layers, SigmoidMSELoss(), Sigmoid())
# layers = [
#             FullyConnected(2, 2),
#             Sigmoid(),
#             FullyConnected(1, 2),
#           ]
# mymlp = MultiLayerPerceptron(layers, SigmoidCrossEntropyLoss(), Sigmoid())

print(mymlp)

x = [[0, 0],
     [0, 1],
     [1, 0],
     [1, 1],]
y = [
     0,
     1,
     1,
     0
     ]

input_x = np.asarray(x,dtype = np.float32)
output_y = np.asarray(y,dtype = np.float32).reshape(-1, 1)
# output_y = (np.random.rand(3, 3)>0.6) * 1

input_x.shape

# mymlp.forward(input_x,output_y)

# mymlp.layers[0].weight, mymlp.layers[0].bias,

# mymlp.all_outputs

# mymlp.backward()

input_x, output_y

losses = []
mymlp.train_mode()
for _ in range(5000):
    _, loss = mymlp.forward(input_x, output_y)
    mymlp.backward()
    print(mymlp.optimize(0.5))
    print('Loss', loss)
    losses.append(loss)

mymlp.eval_mode()
out, _ =  mymlp.forward(input_x)

out[-1]

(out[-1][0] > 0.5) * 1

(out[2] > 0.5) * 1

(out[4][0] > 0.5) * 1

plt.plot(losses)
plt.show()

!wget https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data

iris_data = pd.read_csv('iris.data', header = None)

data_x = iris_data.values[:, : 4].astype('float')
data_y = iris_data.values[:, 4].reshape(-1, 1)

sss = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, random_state = 42)
sss.get_n_splits(data_x, data_y)
#iris_data = iris_data.sample(frac = 1).reset_index(drop = True)

train_val_index, test_index = list(sss.split(data_x, data_y))[0]
x_train_val, x_test = data_x[train_val_index], data_x[test_index]
y_train_val, y_test = data_y[train_val_index], data_y[test_index]

sss = StratifiedShuffleSplit(n_splits = 1, test_size = 0.25, random_state = 42)
sss.get_n_splits(x_train_val, y_train_val)

train_index, val_index = list(sss.split(x_train_val, y_train_val))[0]
x_train, x_val = x_train_val[train_index], x_train_val[val_index]
y_train, y_val = y_train_val[train_index], y_train_val[val_index]

print(len(x_train), len(x_val), len(x_test))
print(len(y_train), len(y_val), len(y_test))

labels2id = {name: i for i, name in enumerate(set(data_y[:, 0]))}

labels2id

onehotencode = lambda i: np.eye(3)[labels2id[i]]
y_train_one_hot = np.asarray([onehotencode(i) for i in y_train[:, 0]])
y_val_one_hot = np.asarray([onehotencode(i) for i in y_val[:, 0]])
y_test_one_hot = np.asarray([onehotencode(i) for i in y_test[:, 0]])

print(x_train.shape, x_val.shape, x_test.shape)
print(y_train_one_hot.shape, y_val_one_hot.shape, y_test_one_hot.shape)

layers_iris = [
                FullyConnected(6, 4),
                Sigmoid(),
                FullyConnected(3, 6),
              ]
mymlp_iris = MultiLayerPerceptron(layers_iris, SoftmaxCrossEntropyLoss(), SoftmaxLayer())

print(mymlp_iris)

# best_mlp_iris = None
# best_scores = 0.0
# for _ in range(500):
#     mymlp_iris.train_mode()
#     _, loss = mymlp_iris.forward(x_train, y_train_one_hot)
#     mymlp_iris.backward()
#     print(mymlp_iris.optimize(1))
#     print('Loss',loss)

from sklearn.metrics import precision_recall_fscore_support

def tester(model, x, y_true):
    model.eval_mode()
    out, _ = model.forward(x)
    model.train_mode()
    predictions = np.argmax(out[-1][0], axis=1)
    ground_truth = np.argmax(y_true, axis=1)
    acc = np.mean(predictions == ground_truth)
    precision, recall, fscore, _ = precision_recall_fscore_support(ground_truth, predictions, average='weighted')
    return acc, precision, recall, fscore



tester(mymlp_iris, x_val, y_val_one_hot)

tester(mymlp_iris, x_test, y_test_one_hot)

best_mlp_iris = None
best_f1 = 0.0
losses = []
for epoch in range(500):
    mymlp_iris.train_mode()
    _, loss = mymlp_iris.forward(x_train, y_train_one_hot)
    mymlp_iris.backward()

    train_acc, train_prec, train_rec, train_f1 = tester(mymlp_iris, x_train, y_train_one_hot)
    print("Epoch {} train_acc: {}, train_prec: {}, train_rec: {}, train_f1: {}".format(epoch, train_acc, train_prec, train_rec, train_f1))

    val_acc, val_prec, val_rec, val_f1 = tester(mymlp_iris, x_val, y_val_one_hot)
    print("Epoch {} val_acc: {}, val_prec: {}, val_rec: {}, val_f1: {}".format(epoch, val_acc, val_prec, val_rec, val_f1))

    test_acc, test_prec, test_rec, test_f1 = tester(mymlp_iris, x_test, y_test_one_hot)
    print("Epoch {} test_acc: {}, test_prec: {}, test_rec: {}, test_f1: {}".format(epoch, test_acc, test_prec, test_rec, test_f1))

    if  val_f1 > best_f1:
        best_mlp_iris = deepcopy(mymlp_iris)
        best_f1 = val_f1
        print("Saving as best model...")
    print(mymlp_iris.optimize(1))
    print('Loss',loss)
    losses.append(loss)

plt.plot(losses)
plt.show()

test_acc, test_prec, test_rec, test_f1 = tester(best_mlp_iris, x_test, y_test_one_hot)
print("TEST: test_acc: {}, test_prec: {}, test_rec: {}, test_f1: {}".format(test_acc, test_prec, test_rec, test_f1))

!rm -rf *
!wget http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz
!wget http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz
!wget http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz
!wget http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz

!gunzip *.gz

from mlxtend.data import loadlocal_mnist

x_train, y_train = loadlocal_mnist(
        images_path='train-images-idx3-ubyte', 
        labels_path='train-labels-idx1-ubyte')

print('Dimensions: %s x %s' % (x_train.shape[0], x_train.shape[1]))

x_train = x_train.astype('float')
y_train = y_train.astype('float')

x_test, y_test = loadlocal_mnist(
        images_path='t10k-images-idx3-ubyte', 
        labels_path='t10k-labels-idx1-ubyte')

x_test = x_test.astype('float')
y_test = y_test.astype('float')

sss = StratifiedShuffleSplit(n_splits = 1, test_size = (1.0/6.0), random_state = 42)
sss.get_n_splits(x_train, y_train)
train_idx, valid_idx = next(sss.split(x_train, y_train))

len(train_idx), len(valid_idx)

x_train, y_train, x_val, y_val = x_train[train_idx], y_train[train_idx], x_train[valid_idx], y_train[valid_idx]

onehotencode = lambda i: np.eye(10)[int(i)]
y_train_one_hot = np.asarray([onehotencode(i) for i in y_train])
y_val_one_hot = np.asarray([onehotencode(i) for i in y_val])
y_test_one_hot = np.asarray([onehotencode(i) for i in y_test])

# Normalize inputs. Calculate mean and standard deviation on only training data, then apply on test too.
train_mean = np.mean(x_train)
std = np.std(x_train)
print(train_mean, std)

x_train = (x_train - train_mean) / (std)
x_val = (x_val - train_mean) / (std)
x_test = (x_test - train_mean) / (std)

print(np.mean(x_train), np.std(x_train))
print(np.mean(x_val), np.std(x_val))
print(np.mean(x_test), np.std(x_test))

# layers_mnist = [
#                 FullyConnected(16, 784),
#                 Sigmoid(),
#                 FullyConnected(10, 16),
#               ]
layers_mnist = [
                FullyConnected(16, 784),
                Tanh(),
                FullyConnected(10, 16),
              ]
mymlp_mnist = MultiLayerPerceptron(layers_mnist, SoftmaxCrossEntropyLoss(), SoftmaxLayer())

del mymlp_iris

best_mlp_mnist = None
best_f1 = 0.0
losses = []
for epoch in range(100):
    mymlp_mnist.train_mode()
    _, loss = mymlp_mnist.forward(x_train, y_train_one_hot)
    mymlp_mnist.backward()

    train_acc, train_prec, train_rec, train_f1 = tester(mymlp_mnist, x_train, y_train_one_hot)
    print("Epoch {} train_acc: {}, train_prec: {}, train_rec: {}, train_f1: {}".format(epoch, train_acc, train_prec, train_rec, train_f1))

    val_acc, val_prec, val_rec, val_f1 = tester(mymlp_mnist, x_val, y_val_one_hot)
    print("Epoch {} val_acc: {}, val_prec: {}, val_rec: {}, val_f1: {}".format(epoch, val_acc, val_prec, val_rec, val_f1))

    test_acc, test_prec, test_rec, test_f1 = tester(mymlp_mnist, x_test, y_test_one_hot)
    print("Epoch {} test_acc: {}, test_prec: {}, test_rec: {}, test_f1: {}".format(epoch, test_acc, test_prec, test_rec, test_f1))

    if  val_f1 > best_f1:
        best_mlp_mnist = deepcopy(mymlp_mnist)
        best_f1 = val_f1
        print("Saving as best model...")
    print(mymlp_mnist.optimize(1.0))
    print('Loss',loss)
    losses.append(loss)

test_acc, test_prec, test_rec, test_f1 = tester(best_mlp_mnist, x_test, y_test_one_hot)
print("TEST: test_acc: {}, test_prec: {}, test_rec: {}, test_f1: {}".format(test_acc, test_prec, test_rec, test_f1))

mymlp_mnist.all_outputs

plt.plot(losses)
plt.show()

